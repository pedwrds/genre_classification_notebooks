{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-12 14:32:23.761518: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier as skKerasClassifier\n",
    "# from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "# import keras.api._v2.keras as keras #noqa\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras import activations\n",
    "# from keras.losses import CategoricalCrossentropy\n",
    "from keras.callbacks import History, EarlyStopping, ModelCheckpoint\n",
    "# from keras import regularizers\n",
    "# import keras.backend as K\n",
    "\n",
    "from genreclassification.utils import get_project_root\n",
    "\n",
    "import dataframe_image as dfi\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test best model - retrained on entire training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = pd.read_csv(\n",
    "    get_project_root() / \"data/features_3_sec.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### features / targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the target labels:\n",
    "df_3_y = df_3[\"label\"]\n",
    "\n",
    "# find the training features:\n",
    "df_3_x = df_3.drop(\n",
    "    columns=[\"filename\", \"length\", \"label\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test split before scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df_3_x,\n",
    "    df_3_y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (7992, 57)\n",
      "y_train: (7992,)\n",
      " x_test: (1998, 57)\n",
      " y_test: (1998,)\n"
     ]
    }
   ],
   "source": [
    "for d in [\n",
    "    (x_train, \"x_train\"),\n",
    "    (y_train, \"y_train\"),\n",
    "    (x_test, \" x_test\"),\n",
    "    (y_test, \" y_test\")\n",
    "]:\n",
    "    print(f\"{d[1]}: {d[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMax scaling for the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler((0,1))\n",
    "# scale fetures and get column names\n",
    "scale_cols = x_train.columns\n",
    "x_scaled = scaler.fit_transform(x_train[scale_cols])\n",
    "# retrieve column names for scaled df:\n",
    "x_train_scaled = pd.DataFrame(\n",
    "    x_scaled,\n",
    "    columns=scale_cols\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoding the categorical classes\n",
    "* the model will produce a probability score for each of the 10 classes, assigning the most likely label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by mapping the labels to numerical values:\n",
    "catno_to_label = {key:value for (key, value) in enumerate(sorted(set(y_train)))}\n",
    "# reverse:\n",
    "label_to_catno = {v:k for k,v in catno_to_label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.map(label_to_catno)\n",
    "\n",
    "label_as_binary = LabelBinarizer()\n",
    "y_train = label_as_binary.fit_transform(y_train)\n",
    "\n",
    "y_train = pd.DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (7992, 57)\n",
      "y_train: (7992, 10)\n"
     ]
    }
   ],
   "source": [
    "for d in [\n",
    "    (x_train, \"x_train\"),\n",
    "    # (x_val, \"  x_val\"),\n",
    "    (y_train, \"y_train\"),\n",
    "    # (y_val, \"  y_val\")\n",
    "]:\n",
    "    print(f\"{d[1]}: {d[0].shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train NN from here or jump to [testing](#performance-on-test-set) to load a saved model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## define model functions ready for gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlnn(\n",
    "    hidden_dim=128,\n",
    "    activation_fn=activations.selu,\n",
    "    dropout_rate=0.2\n",
    "):\n",
    "    model = Sequential(name=\"mlnn_genre_classification\")\n",
    "    #input layer\n",
    "    model.add(Dense(\n",
    "        units=32,\n",
    "        activation=activation_fn,\n",
    "        # kernel_regularizer=regularizers.l2(0.01),\n",
    "        input_shape=(x_train_scaled.shape[1],),\n",
    "        name=\"input\")\n",
    "    )\n",
    "    # batch normalistion:\n",
    "    model.add(BatchNormalization())\n",
    "    # dropout\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    # hidden layer\n",
    "    model.add(Dense(\n",
    "        units=hidden_dim,\n",
    "        activation=activation_fn,\n",
    "        # kernel_regularizer=regularizers.l2(0.01),\n",
    "        name=\"hidden1\"\n",
    "    ))\n",
    "    # batch normalistion:\n",
    "    model.add(BatchNormalization())\n",
    "    # dropout\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    # hidden layer\n",
    "    model.add(Dense(\n",
    "        units=hidden_dim,\n",
    "        activation=activation_fn,\n",
    "        # kernel_regularizer=regularizers.l2(0.01),\n",
    "        name=\"hidden2\"\n",
    "    ))\n",
    "    # batch normalistion:\n",
    "    model.add(BatchNormalization())\n",
    "    # dropout\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    # hidden layer\n",
    "    model.add(Dense(\n",
    "        units=hidden_dim,\n",
    "        activation=\"selu\",\n",
    "        # kernel_regularizer=regularizers.l2(0.01),\n",
    "        name=\"hidden3\"\n",
    "    ))\n",
    "    # batch normalistion:\n",
    "    model.add(BatchNormalization())\n",
    "    # dropout\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    # output layer\n",
    "    model.add(Dense(\n",
    "        units=10,\n",
    "        activation=\"softmax\",\n",
    "        name=\"output\"\n",
    "    ))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlnn_compile(\n",
    "    model,\n",
    "    optimiser=optimizers.Adam,\n",
    "    learning_rate=0.0001\n",
    "):\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=optimiser(learning_rate=learning_rate),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fit:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* adjust the fit function to monitor loss and not validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlnn_fit(\n",
    "    model,\n",
    "    x,\n",
    "    y,\n",
    "    val_xy=None,\n",
    "    epochs=700,\n",
    "    batch_size=32,\n",
    "):\n",
    "    history=History()\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor=\"loss\",\n",
    "        min_delta=0,\n",
    "        restore_best_weights=True,\n",
    "        patience=epochs,\n",
    "        baseline=None\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        x,\n",
    "        y,\n",
    "        validation_data=val_xy,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1,\n",
    "        callbacks=[\n",
    "            history,\n",
    "            early_stopping\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build, compile, fit model on all training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1300\n",
      "250/250 [==============================] - 2s 3ms/step - loss: 1.4222 - accuracy: 0.4975\n",
      "Epoch 2/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 1.1456 - accuracy: 0.5953\n",
      "Epoch 3/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 1.0276 - accuracy: 0.6369\n",
      "Epoch 4/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.9615 - accuracy: 0.6655\n",
      "Epoch 5/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.9481 - accuracy: 0.6692\n",
      "Epoch 6/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.9074 - accuracy: 0.6814\n",
      "Epoch 7/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.8625 - accuracy: 0.6944\n",
      "Epoch 8/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.8435 - accuracy: 0.7027\n",
      "Epoch 9/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.8249 - accuracy: 0.7111\n",
      "Epoch 10/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.7996 - accuracy: 0.7162\n",
      "Epoch 11/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.7882 - accuracy: 0.7261\n",
      "Epoch 12/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.7670 - accuracy: 0.7280\n",
      "Epoch 13/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.7483 - accuracy: 0.7322\n",
      "Epoch 14/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.7216 - accuracy: 0.7467\n",
      "Epoch 15/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.7118 - accuracy: 0.7544\n",
      "Epoch 16/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.7018 - accuracy: 0.7466\n",
      "Epoch 17/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.6991 - accuracy: 0.7546\n",
      "Epoch 18/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.6789 - accuracy: 0.7600\n",
      "Epoch 19/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.6623 - accuracy: 0.7655\n",
      "Epoch 20/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.6547 - accuracy: 0.7752\n",
      "Epoch 21/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.7654\n",
      "Epoch 22/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.6218 - accuracy: 0.7767\n",
      "Epoch 23/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.6190 - accuracy: 0.7849\n",
      "Epoch 24/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.6279 - accuracy: 0.7802\n",
      "Epoch 25/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.6064 - accuracy: 0.7880\n",
      "Epoch 26/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.5965 - accuracy: 0.7903\n",
      "Epoch 27/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.5910 - accuracy: 0.7917\n",
      "Epoch 28/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.5807 - accuracy: 0.7925\n",
      "Epoch 29/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.5716 - accuracy: 0.8007\n",
      "Epoch 30/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.5705 - accuracy: 0.7989\n",
      "Epoch 31/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.5731 - accuracy: 0.7913\n",
      "Epoch 32/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.5455 - accuracy: 0.8118\n",
      "Epoch 33/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.5552 - accuracy: 0.8072\n",
      "Epoch 34/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.5348 - accuracy: 0.8127\n",
      "Epoch 35/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.5478 - accuracy: 0.8063\n",
      "Epoch 36/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.5219 - accuracy: 0.8197\n",
      "Epoch 37/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.5305 - accuracy: 0.8124\n",
      "Epoch 38/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.5213 - accuracy: 0.8131\n",
      "Epoch 39/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.5026 - accuracy: 0.8207\n",
      "Epoch 40/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.5043 - accuracy: 0.8196\n",
      "Epoch 41/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.5036 - accuracy: 0.8177\n",
      "Epoch 42/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4996 - accuracy: 0.8262\n",
      "Epoch 43/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4979 - accuracy: 0.8238\n",
      "Epoch 44/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4894 - accuracy: 0.8275\n",
      "Epoch 45/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.5006 - accuracy: 0.8255\n",
      "Epoch 46/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4794 - accuracy: 0.8312\n",
      "Epoch 47/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4683 - accuracy: 0.8330\n",
      "Epoch 48/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4696 - accuracy: 0.8362\n",
      "Epoch 49/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4651 - accuracy: 0.8355\n",
      "Epoch 50/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4681 - accuracy: 0.8345\n",
      "Epoch 51/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4535 - accuracy: 0.8416\n",
      "Epoch 52/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4557 - accuracy: 0.8417\n",
      "Epoch 53/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4521 - accuracy: 0.8388\n",
      "Epoch 54/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4598 - accuracy: 0.8358\n",
      "Epoch 55/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4472 - accuracy: 0.8416\n",
      "Epoch 56/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4499 - accuracy: 0.8422\n",
      "Epoch 57/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4525 - accuracy: 0.8411\n",
      "Epoch 58/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4356 - accuracy: 0.8482\n",
      "Epoch 59/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4380 - accuracy: 0.8485\n",
      "Epoch 60/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4354 - accuracy: 0.8456\n",
      "Epoch 61/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4205 - accuracy: 0.8555\n",
      "Epoch 62/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4352 - accuracy: 0.8448\n",
      "Epoch 63/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4337 - accuracy: 0.8466\n",
      "Epoch 64/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4225 - accuracy: 0.8477\n",
      "Epoch 65/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4201 - accuracy: 0.8535\n",
      "Epoch 66/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4135 - accuracy: 0.8511\n",
      "Epoch 67/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4137 - accuracy: 0.8524\n",
      "Epoch 68/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4215 - accuracy: 0.8529\n",
      "Epoch 69/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4026 - accuracy: 0.8574\n",
      "Epoch 70/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3972 - accuracy: 0.8615\n",
      "Epoch 71/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4139 - accuracy: 0.8554\n",
      "Epoch 72/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4084 - accuracy: 0.8586\n",
      "Epoch 73/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4055 - accuracy: 0.8565\n",
      "Epoch 74/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3924 - accuracy: 0.8594\n",
      "Epoch 75/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3960 - accuracy: 0.8545\n",
      "Epoch 76/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3998 - accuracy: 0.8580\n",
      "Epoch 77/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3827 - accuracy: 0.8617\n",
      "Epoch 78/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3971 - accuracy: 0.8542\n",
      "Epoch 79/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3909 - accuracy: 0.8615\n",
      "Epoch 80/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3842 - accuracy: 0.8622\n",
      "Epoch 81/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3746 - accuracy: 0.8666\n",
      "Epoch 82/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3865 - accuracy: 0.8642\n",
      "Epoch 83/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3730 - accuracy: 0.8686\n",
      "Epoch 84/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3721 - accuracy: 0.8675\n",
      "Epoch 85/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3534 - accuracy: 0.8736\n",
      "Epoch 86/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3598 - accuracy: 0.8702\n",
      "Epoch 87/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3797 - accuracy: 0.8674\n",
      "Epoch 88/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3676 - accuracy: 0.8679\n",
      "Epoch 89/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3691 - accuracy: 0.8711\n",
      "Epoch 90/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3705 - accuracy: 0.8680\n",
      "Epoch 91/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3633 - accuracy: 0.8681\n",
      "Epoch 92/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3814 - accuracy: 0.8649\n",
      "Epoch 93/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3643 - accuracy: 0.8695\n",
      "Epoch 94/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3525 - accuracy: 0.8724\n",
      "Epoch 95/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3526 - accuracy: 0.8785\n",
      "Epoch 96/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3531 - accuracy: 0.8721\n",
      "Epoch 97/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3421 - accuracy: 0.8770\n",
      "Epoch 98/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3462 - accuracy: 0.8739\n",
      "Epoch 99/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3343 - accuracy: 0.8841\n",
      "Epoch 100/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3379 - accuracy: 0.8784\n",
      "Epoch 101/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3454 - accuracy: 0.8769\n",
      "Epoch 102/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3414 - accuracy: 0.8809\n",
      "Epoch 103/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3571 - accuracy: 0.8686\n",
      "Epoch 104/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3374 - accuracy: 0.8764\n",
      "Epoch 105/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3309 - accuracy: 0.8819\n",
      "Epoch 106/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3414 - accuracy: 0.8784\n",
      "Epoch 107/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3216 - accuracy: 0.8885\n",
      "Epoch 108/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3481 - accuracy: 0.8775\n",
      "Epoch 109/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3340 - accuracy: 0.8840\n",
      "Epoch 110/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3381 - accuracy: 0.8805\n",
      "Epoch 111/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3408 - accuracy: 0.8794\n",
      "Epoch 112/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3415 - accuracy: 0.8778\n",
      "Epoch 113/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3376 - accuracy: 0.8816\n",
      "Epoch 114/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3258 - accuracy: 0.8898\n",
      "Epoch 115/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3291 - accuracy: 0.8834\n",
      "Epoch 116/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3212 - accuracy: 0.8854\n",
      "Epoch 117/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3284 - accuracy: 0.8864\n",
      "Epoch 118/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3293 - accuracy: 0.8840\n",
      "Epoch 119/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3284 - accuracy: 0.8846\n",
      "Epoch 120/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3262 - accuracy: 0.8858\n",
      "Epoch 121/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3157 - accuracy: 0.8911\n",
      "Epoch 122/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3395 - accuracy: 0.8828\n",
      "Epoch 123/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3335 - accuracy: 0.8821\n",
      "Epoch 124/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3204 - accuracy: 0.8855\n",
      "Epoch 125/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3122 - accuracy: 0.8896\n",
      "Epoch 126/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3175 - accuracy: 0.8870\n",
      "Epoch 127/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3260 - accuracy: 0.8866\n",
      "Epoch 128/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3158 - accuracy: 0.8886\n",
      "Epoch 129/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3139 - accuracy: 0.8876\n",
      "Epoch 130/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3022 - accuracy: 0.8921\n",
      "Epoch 131/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3241 - accuracy: 0.8856\n",
      "Epoch 132/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3115 - accuracy: 0.8906\n",
      "Epoch 133/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3178 - accuracy: 0.8874\n",
      "Epoch 134/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2967 - accuracy: 0.8923\n",
      "Epoch 135/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3071 - accuracy: 0.8880\n",
      "Epoch 136/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3051 - accuracy: 0.8928\n",
      "Epoch 137/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3141 - accuracy: 0.8876\n",
      "Epoch 138/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3065 - accuracy: 0.8890\n",
      "Epoch 139/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3182 - accuracy: 0.8901\n",
      "Epoch 140/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3103 - accuracy: 0.8913\n",
      "Epoch 141/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3174 - accuracy: 0.8845\n",
      "Epoch 142/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3139 - accuracy: 0.8881\n",
      "Epoch 143/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2970 - accuracy: 0.8961\n",
      "Epoch 144/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3062 - accuracy: 0.8931\n",
      "Epoch 145/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3087 - accuracy: 0.8926\n",
      "Epoch 146/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3069 - accuracy: 0.8940\n",
      "Epoch 147/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2917 - accuracy: 0.8933\n",
      "Epoch 148/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3192 - accuracy: 0.8855\n",
      "Epoch 149/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2962 - accuracy: 0.8990\n",
      "Epoch 150/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3119 - accuracy: 0.8920\n",
      "Epoch 151/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2887 - accuracy: 0.8984\n",
      "Epoch 152/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2758 - accuracy: 0.9038\n",
      "Epoch 153/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2944 - accuracy: 0.8985\n",
      "Epoch 154/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2856 - accuracy: 0.9008\n",
      "Epoch 155/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3032 - accuracy: 0.8926\n",
      "Epoch 156/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2943 - accuracy: 0.8946\n",
      "Epoch 157/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3036 - accuracy: 0.8960\n",
      "Epoch 158/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2925 - accuracy: 0.8940\n",
      "Epoch 159/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2778 - accuracy: 0.9032\n",
      "Epoch 160/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2878 - accuracy: 0.8994\n",
      "Epoch 161/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2851 - accuracy: 0.9013\n",
      "Epoch 162/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2910 - accuracy: 0.8959\n",
      "Epoch 163/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3030 - accuracy: 0.8951\n",
      "Epoch 164/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2839 - accuracy: 0.8998\n",
      "Epoch 165/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2952 - accuracy: 0.8965\n",
      "Epoch 166/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2894 - accuracy: 0.8989\n",
      "Epoch 167/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2915 - accuracy: 0.8988\n",
      "Epoch 168/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2876 - accuracy: 0.8986\n",
      "Epoch 169/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2694 - accuracy: 0.9080\n",
      "Epoch 170/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2943 - accuracy: 0.8984\n",
      "Epoch 171/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2807 - accuracy: 0.9010\n",
      "Epoch 172/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2869 - accuracy: 0.8988\n",
      "Epoch 173/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2803 - accuracy: 0.8999\n",
      "Epoch 174/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2824 - accuracy: 0.9060\n",
      "Epoch 175/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2787 - accuracy: 0.8984\n",
      "Epoch 176/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2635 - accuracy: 0.9054\n",
      "Epoch 177/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2768 - accuracy: 0.9010\n",
      "Epoch 178/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2735 - accuracy: 0.9063\n",
      "Epoch 179/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2816 - accuracy: 0.9022\n",
      "Epoch 180/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2826 - accuracy: 0.9010\n",
      "Epoch 181/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2749 - accuracy: 0.9042\n",
      "Epoch 182/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2657 - accuracy: 0.9068\n",
      "Epoch 183/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2711 - accuracy: 0.9054\n",
      "Epoch 184/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2694 - accuracy: 0.9049\n",
      "Epoch 185/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2766 - accuracy: 0.9029\n",
      "Epoch 186/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2544 - accuracy: 0.9099\n",
      "Epoch 187/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2731 - accuracy: 0.9057\n",
      "Epoch 188/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2794 - accuracy: 0.9044\n",
      "Epoch 189/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2789 - accuracy: 0.9038\n",
      "Epoch 190/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2661 - accuracy: 0.9050\n",
      "Epoch 191/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2734 - accuracy: 0.9025\n",
      "Epoch 192/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2759 - accuracy: 0.9045\n",
      "Epoch 193/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2609 - accuracy: 0.9084\n",
      "Epoch 194/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2661 - accuracy: 0.9070\n",
      "Epoch 195/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2695 - accuracy: 0.9074\n",
      "Epoch 196/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2668 - accuracy: 0.9067\n",
      "Epoch 197/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2598 - accuracy: 0.9108\n",
      "Epoch 198/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2693 - accuracy: 0.9064\n",
      "Epoch 199/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2687 - accuracy: 0.9082\n",
      "Epoch 200/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2707 - accuracy: 0.9035\n",
      "Epoch 201/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2639 - accuracy: 0.9088\n",
      "Epoch 202/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2589 - accuracy: 0.9103\n",
      "Epoch 203/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2641 - accuracy: 0.9078\n",
      "Epoch 204/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2579 - accuracy: 0.9100\n",
      "Epoch 205/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2525 - accuracy: 0.9105\n",
      "Epoch 206/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2674 - accuracy: 0.9053\n",
      "Epoch 207/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2556 - accuracy: 0.9109\n",
      "Epoch 208/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2586 - accuracy: 0.9094\n",
      "Epoch 209/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2640 - accuracy: 0.9028\n",
      "Epoch 210/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2589 - accuracy: 0.9119\n",
      "Epoch 211/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2565 - accuracy: 0.9122\n",
      "Epoch 212/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2506 - accuracy: 0.9094\n",
      "Epoch 213/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2443 - accuracy: 0.9170\n",
      "Epoch 214/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2588 - accuracy: 0.9117\n",
      "Epoch 215/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2557 - accuracy: 0.9124\n",
      "Epoch 216/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2473 - accuracy: 0.9144\n",
      "Epoch 217/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2493 - accuracy: 0.9113\n",
      "Epoch 218/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2668 - accuracy: 0.9060\n",
      "Epoch 219/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2617 - accuracy: 0.9080\n",
      "Epoch 220/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2720 - accuracy: 0.9054\n",
      "Epoch 221/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2604 - accuracy: 0.9089\n",
      "Epoch 222/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2577 - accuracy: 0.9088\n",
      "Epoch 223/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2603 - accuracy: 0.9069\n",
      "Epoch 224/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2428 - accuracy: 0.9144\n",
      "Epoch 225/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2472 - accuracy: 0.9109\n",
      "Epoch 226/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2709 - accuracy: 0.9047\n",
      "Epoch 227/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2610 - accuracy: 0.9107\n",
      "Epoch 228/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2546 - accuracy: 0.9083\n",
      "Epoch 229/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2555 - accuracy: 0.9104\n",
      "Epoch 230/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2671 - accuracy: 0.9063\n",
      "Epoch 231/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2591 - accuracy: 0.9088\n",
      "Epoch 232/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2551 - accuracy: 0.9120\n",
      "Epoch 233/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2434 - accuracy: 0.9144\n",
      "Epoch 234/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2516 - accuracy: 0.9122\n",
      "Epoch 235/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2562 - accuracy: 0.9090\n",
      "Epoch 236/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2453 - accuracy: 0.9140\n",
      "Epoch 237/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2471 - accuracy: 0.9129\n",
      "Epoch 238/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2488 - accuracy: 0.9105\n",
      "Epoch 239/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2633 - accuracy: 0.9054\n",
      "Epoch 240/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2461 - accuracy: 0.9177\n",
      "Epoch 241/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2610 - accuracy: 0.9094\n",
      "Epoch 242/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2413 - accuracy: 0.9155\n",
      "Epoch 243/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2400 - accuracy: 0.9144\n",
      "Epoch 244/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2344 - accuracy: 0.9158\n",
      "Epoch 245/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2460 - accuracy: 0.9153\n",
      "Epoch 246/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2389 - accuracy: 0.9165\n",
      "Epoch 247/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2436 - accuracy: 0.9127\n",
      "Epoch 248/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2480 - accuracy: 0.9145\n",
      "Epoch 249/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2355 - accuracy: 0.9169\n",
      "Epoch 250/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2538 - accuracy: 0.9119\n",
      "Epoch 251/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2431 - accuracy: 0.9154\n",
      "Epoch 252/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2450 - accuracy: 0.9180\n",
      "Epoch 253/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2430 - accuracy: 0.9158\n",
      "Epoch 254/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2514 - accuracy: 0.9128\n",
      "Epoch 255/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2522 - accuracy: 0.9099\n",
      "Epoch 256/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2520 - accuracy: 0.9135\n",
      "Epoch 257/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2437 - accuracy: 0.9162\n",
      "Epoch 258/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2580 - accuracy: 0.9112\n",
      "Epoch 259/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.9117\n",
      "Epoch 260/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2385 - accuracy: 0.9153\n",
      "Epoch 261/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2417 - accuracy: 0.9172\n",
      "Epoch 262/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2498 - accuracy: 0.9094\n",
      "Epoch 263/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2446 - accuracy: 0.9147\n",
      "Epoch 264/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2481 - accuracy: 0.9170\n",
      "Epoch 265/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2453 - accuracy: 0.9138\n",
      "Epoch 266/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2476 - accuracy: 0.9135\n",
      "Epoch 267/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2620 - accuracy: 0.9082\n",
      "Epoch 268/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2441 - accuracy: 0.9119\n",
      "Epoch 269/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2395 - accuracy: 0.9155\n",
      "Epoch 270/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2347 - accuracy: 0.9154\n",
      "Epoch 271/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2442 - accuracy: 0.9134\n",
      "Epoch 272/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2464 - accuracy: 0.9100\n",
      "Epoch 273/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2386 - accuracy: 0.9175\n",
      "Epoch 274/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2404 - accuracy: 0.9140\n",
      "Epoch 275/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2444 - accuracy: 0.9155\n",
      "Epoch 276/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2388 - accuracy: 0.9192\n",
      "Epoch 277/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2433 - accuracy: 0.9135\n",
      "Epoch 278/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2422 - accuracy: 0.9159\n",
      "Epoch 279/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2487 - accuracy: 0.9127\n",
      "Epoch 280/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2431 - accuracy: 0.9184\n",
      "Epoch 281/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2383 - accuracy: 0.9170\n",
      "Epoch 282/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2345 - accuracy: 0.9180\n",
      "Epoch 283/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2560 - accuracy: 0.9105\n",
      "Epoch 284/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2366 - accuracy: 0.9182\n",
      "Epoch 285/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2243 - accuracy: 0.9228\n",
      "Epoch 286/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2325 - accuracy: 0.9187\n",
      "Epoch 287/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2284 - accuracy: 0.9200\n",
      "Epoch 288/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2266 - accuracy: 0.9213\n",
      "Epoch 289/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2425 - accuracy: 0.9155\n",
      "Epoch 290/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2397 - accuracy: 0.9169\n",
      "Epoch 291/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2321 - accuracy: 0.9178\n",
      "Epoch 292/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2354 - accuracy: 0.9158\n",
      "Epoch 293/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2374 - accuracy: 0.9180\n",
      "Epoch 294/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2319 - accuracy: 0.9199\n",
      "Epoch 295/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2487 - accuracy: 0.9144\n",
      "Epoch 296/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2327 - accuracy: 0.9192\n",
      "Epoch 297/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2302 - accuracy: 0.9207\n",
      "Epoch 298/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2354 - accuracy: 0.9185\n",
      "Epoch 299/1300\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.2307 - accuracy: 0.9152\n",
      "Epoch 300/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2251 - accuracy: 0.9233\n",
      "Epoch 301/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2312 - accuracy: 0.9198\n",
      "Epoch 302/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2302 - accuracy: 0.9205\n",
      "Epoch 303/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2074 - accuracy: 0.9297\n",
      "Epoch 304/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2142 - accuracy: 0.9264\n",
      "Epoch 305/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2133 - accuracy: 0.9268\n",
      "Epoch 306/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2369 - accuracy: 0.9163\n",
      "Epoch 307/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2273 - accuracy: 0.9199\n",
      "Epoch 308/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2330 - accuracy: 0.9178\n",
      "Epoch 309/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2288 - accuracy: 0.9224\n",
      "Epoch 310/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2298 - accuracy: 0.9175\n",
      "Epoch 311/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2355 - accuracy: 0.9185\n",
      "Epoch 312/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2233 - accuracy: 0.9184\n",
      "Epoch 313/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2215 - accuracy: 0.9245\n",
      "Epoch 314/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2261 - accuracy: 0.9218\n",
      "Epoch 315/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2258 - accuracy: 0.9197\n",
      "Epoch 316/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2361 - accuracy: 0.9172\n",
      "Epoch 317/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2488 - accuracy: 0.9148\n",
      "Epoch 318/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2301 - accuracy: 0.9197\n",
      "Epoch 319/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2119 - accuracy: 0.9258\n",
      "Epoch 320/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2335 - accuracy: 0.9225\n",
      "Epoch 321/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2309 - accuracy: 0.9198\n",
      "Epoch 322/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2084 - accuracy: 0.9287\n",
      "Epoch 323/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2351 - accuracy: 0.9169\n",
      "Epoch 324/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2241 - accuracy: 0.9217\n",
      "Epoch 325/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2215 - accuracy: 0.9207\n",
      "Epoch 326/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2247 - accuracy: 0.9220\n",
      "Epoch 327/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2301 - accuracy: 0.9188\n",
      "Epoch 328/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2344 - accuracy: 0.9205\n",
      "Epoch 329/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2361 - accuracy: 0.9190\n",
      "Epoch 330/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2213 - accuracy: 0.9207\n",
      "Epoch 331/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2205 - accuracy: 0.9215\n",
      "Epoch 332/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2163 - accuracy: 0.9243\n",
      "Epoch 333/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2327 - accuracy: 0.9184\n",
      "Epoch 334/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2224 - accuracy: 0.9219\n",
      "Epoch 335/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2281 - accuracy: 0.9208\n",
      "Epoch 336/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2215 - accuracy: 0.9229\n",
      "Epoch 337/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2242 - accuracy: 0.9224\n",
      "Epoch 338/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2154 - accuracy: 0.9233\n",
      "Epoch 339/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2247 - accuracy: 0.9195\n",
      "Epoch 340/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2307 - accuracy: 0.9157\n",
      "Epoch 341/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2356 - accuracy: 0.9213\n",
      "Epoch 342/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2378 - accuracy: 0.9180\n",
      "Epoch 343/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2262 - accuracy: 0.9213\n",
      "Epoch 344/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2141 - accuracy: 0.9251\n",
      "Epoch 345/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2205 - accuracy: 0.9214\n",
      "Epoch 346/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2337 - accuracy: 0.9172\n",
      "Epoch 347/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2252 - accuracy: 0.9217\n",
      "Epoch 348/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2235 - accuracy: 0.9218\n",
      "Epoch 349/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2241 - accuracy: 0.9247\n",
      "Epoch 350/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2226 - accuracy: 0.9239\n",
      "Epoch 351/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2132 - accuracy: 0.9248\n",
      "Epoch 352/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2128 - accuracy: 0.9268\n",
      "Epoch 353/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2225 - accuracy: 0.9251\n",
      "Epoch 354/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2113 - accuracy: 0.9257\n",
      "Epoch 355/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2207 - accuracy: 0.9239\n",
      "Epoch 356/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2231 - accuracy: 0.9212\n",
      "Epoch 357/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2267 - accuracy: 0.9222\n",
      "Epoch 358/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2165 - accuracy: 0.9239\n",
      "Epoch 359/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2265 - accuracy: 0.9212\n",
      "Epoch 360/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2117 - accuracy: 0.9230\n",
      "Epoch 361/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2204 - accuracy: 0.9208\n",
      "Epoch 362/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2152 - accuracy: 0.9266\n",
      "Epoch 363/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2187 - accuracy: 0.9264\n",
      "Epoch 364/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2223 - accuracy: 0.9223\n",
      "Epoch 365/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2101 - accuracy: 0.9284\n",
      "Epoch 366/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2203 - accuracy: 0.9230\n",
      "Epoch 367/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2080 - accuracy: 0.9276\n",
      "Epoch 368/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2227 - accuracy: 0.9237\n",
      "Epoch 369/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2243 - accuracy: 0.9180\n",
      "Epoch 370/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2036 - accuracy: 0.9278\n",
      "Epoch 371/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2122 - accuracy: 0.9248\n",
      "Epoch 372/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2203 - accuracy: 0.9222\n",
      "Epoch 373/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2221 - accuracy: 0.9183\n",
      "Epoch 374/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2363 - accuracy: 0.9198\n",
      "Epoch 375/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2202 - accuracy: 0.9243\n",
      "Epoch 376/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2156 - accuracy: 0.9243\n",
      "Epoch 377/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2126 - accuracy: 0.9258\n",
      "Epoch 378/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2148 - accuracy: 0.9248\n",
      "Epoch 379/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2146 - accuracy: 0.9263\n",
      "Epoch 380/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2164 - accuracy: 0.9239\n",
      "Epoch 381/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2173 - accuracy: 0.9263\n",
      "Epoch 382/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2184 - accuracy: 0.9257\n",
      "Epoch 383/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2253 - accuracy: 0.9220\n",
      "Epoch 384/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2161 - accuracy: 0.9245\n",
      "Epoch 385/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2105 - accuracy: 0.9267\n",
      "Epoch 386/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2088 - accuracy: 0.9284\n",
      "Epoch 387/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2244 - accuracy: 0.9239\n",
      "Epoch 388/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2198 - accuracy: 0.9204\n",
      "Epoch 389/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2113 - accuracy: 0.9257\n",
      "Epoch 390/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2159 - accuracy: 0.9274\n",
      "Epoch 391/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2210 - accuracy: 0.9230\n",
      "Epoch 392/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2203 - accuracy: 0.9219\n",
      "Epoch 393/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2059 - accuracy: 0.9322\n",
      "Epoch 394/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2271 - accuracy: 0.9190\n",
      "Epoch 395/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2120 - accuracy: 0.9239\n",
      "Epoch 396/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2121 - accuracy: 0.9271\n",
      "Epoch 397/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2217 - accuracy: 0.9228\n",
      "Epoch 398/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1984 - accuracy: 0.9298\n",
      "Epoch 399/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2156 - accuracy: 0.9230\n",
      "Epoch 400/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2145 - accuracy: 0.9297\n",
      "Epoch 401/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2236 - accuracy: 0.9264\n",
      "Epoch 402/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2140 - accuracy: 0.9253\n",
      "Epoch 403/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1935 - accuracy: 0.9329\n",
      "Epoch 404/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2041 - accuracy: 0.9247\n",
      "Epoch 405/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2057 - accuracy: 0.9227\n",
      "Epoch 406/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2110 - accuracy: 0.9245\n",
      "Epoch 407/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2151 - accuracy: 0.9257\n",
      "Epoch 408/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2144 - accuracy: 0.9267\n",
      "Epoch 409/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1930 - accuracy: 0.9316\n",
      "Epoch 410/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2169 - accuracy: 0.9230\n",
      "Epoch 411/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2109 - accuracy: 0.9234\n",
      "Epoch 412/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2020 - accuracy: 0.9298\n",
      "Epoch 413/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2153 - accuracy: 0.9251\n",
      "Epoch 414/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1962 - accuracy: 0.9341\n",
      "Epoch 415/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1986 - accuracy: 0.9288\n",
      "Epoch 416/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2064 - accuracy: 0.9296\n",
      "Epoch 417/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2110 - accuracy: 0.9276\n",
      "Epoch 418/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9243\n",
      "Epoch 419/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2029 - accuracy: 0.9309\n",
      "Epoch 420/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2236 - accuracy: 0.9238\n",
      "Epoch 421/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2045 - accuracy: 0.9299\n",
      "Epoch 422/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2006 - accuracy: 0.9313\n",
      "Epoch 423/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2108 - accuracy: 0.9263\n",
      "Epoch 424/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2159 - accuracy: 0.9235\n",
      "Epoch 425/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2011 - accuracy: 0.9294\n",
      "Epoch 426/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1992 - accuracy: 0.9303\n",
      "Epoch 427/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2039 - accuracy: 0.9284\n",
      "Epoch 428/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2042 - accuracy: 0.9278\n",
      "Epoch 429/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1993 - accuracy: 0.9303\n",
      "Epoch 430/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2028 - accuracy: 0.9272\n",
      "Epoch 431/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2036 - accuracy: 0.9301\n",
      "Epoch 432/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2072 - accuracy: 0.9288\n",
      "Epoch 433/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1983 - accuracy: 0.9339\n",
      "Epoch 434/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2114 - accuracy: 0.9268\n",
      "Epoch 435/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2070 - accuracy: 0.9258\n",
      "Epoch 436/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2024 - accuracy: 0.9299\n",
      "Epoch 437/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2123 - accuracy: 0.9257\n",
      "Epoch 438/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2190 - accuracy: 0.9235\n",
      "Epoch 439/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1964 - accuracy: 0.9282\n",
      "Epoch 440/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2044 - accuracy: 0.9291\n",
      "Epoch 441/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2001 - accuracy: 0.9317\n",
      "Epoch 442/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2058 - accuracy: 0.9287\n",
      "Epoch 443/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2214 - accuracy: 0.9220\n",
      "Epoch 444/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1976 - accuracy: 0.9296\n",
      "Epoch 445/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2052 - accuracy: 0.9303\n",
      "Epoch 446/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2002 - accuracy: 0.9296\n",
      "Epoch 447/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2005 - accuracy: 0.9308\n",
      "Epoch 448/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2048 - accuracy: 0.9254\n",
      "Epoch 449/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2063 - accuracy: 0.9286\n",
      "Epoch 450/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2173 - accuracy: 0.9239\n",
      "Epoch 451/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2237 - accuracy: 0.9223\n",
      "Epoch 452/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2049 - accuracy: 0.9251\n",
      "Epoch 453/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1899 - accuracy: 0.9347\n",
      "Epoch 454/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1992 - accuracy: 0.9297\n",
      "Epoch 455/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2157 - accuracy: 0.9240\n",
      "Epoch 456/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1967 - accuracy: 0.9312\n",
      "Epoch 457/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2174 - accuracy: 0.9254\n",
      "Epoch 458/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1944 - accuracy: 0.9312\n",
      "Epoch 459/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1957 - accuracy: 0.9303\n",
      "Epoch 460/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2182 - accuracy: 0.9258\n",
      "Epoch 461/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2020 - accuracy: 0.9274\n",
      "Epoch 462/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1844 - accuracy: 0.9348\n",
      "Epoch 463/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2002 - accuracy: 0.9292\n",
      "Epoch 464/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2062 - accuracy: 0.9222\n",
      "Epoch 465/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2124 - accuracy: 0.9237\n",
      "Epoch 466/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2038 - accuracy: 0.9273\n",
      "Epoch 467/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2071 - accuracy: 0.9272\n",
      "Epoch 468/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1948 - accuracy: 0.9331\n",
      "Epoch 469/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1950 - accuracy: 0.9337\n",
      "Epoch 470/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2059 - accuracy: 0.9278\n",
      "Epoch 471/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2087 - accuracy: 0.9267\n",
      "Epoch 472/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1999 - accuracy: 0.9307\n",
      "Epoch 473/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2070 - accuracy: 0.9274\n",
      "Epoch 474/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1944 - accuracy: 0.9299\n",
      "Epoch 475/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2089 - accuracy: 0.9291\n",
      "Epoch 476/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1954 - accuracy: 0.9303\n",
      "Epoch 477/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2063 - accuracy: 0.9317\n",
      "Epoch 478/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2006 - accuracy: 0.9306\n",
      "Epoch 479/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2005 - accuracy: 0.9319\n",
      "Epoch 480/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1985 - accuracy: 0.9278\n",
      "Epoch 481/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1885 - accuracy: 0.9341\n",
      "Epoch 482/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2058 - accuracy: 0.9296\n",
      "Epoch 483/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1845 - accuracy: 0.9364\n",
      "Epoch 484/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2075 - accuracy: 0.9286\n",
      "Epoch 485/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2068 - accuracy: 0.9288\n",
      "Epoch 486/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1970 - accuracy: 0.9302\n",
      "Epoch 487/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2073 - accuracy: 0.9302\n",
      "Epoch 488/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1934 - accuracy: 0.9337\n",
      "Epoch 489/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1910 - accuracy: 0.9321\n",
      "Epoch 490/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1980 - accuracy: 0.9293\n",
      "Epoch 491/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1908 - accuracy: 0.9334\n",
      "Epoch 492/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1991 - accuracy: 0.9312\n",
      "Epoch 493/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2066 - accuracy: 0.9287\n",
      "Epoch 494/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2036 - accuracy: 0.9307\n",
      "Epoch 495/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1964 - accuracy: 0.9303\n",
      "Epoch 496/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1927 - accuracy: 0.9344\n",
      "Epoch 497/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1908 - accuracy: 0.9344\n",
      "Epoch 498/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1988 - accuracy: 0.9291\n",
      "Epoch 499/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2021 - accuracy: 0.9303\n",
      "Epoch 500/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2010 - accuracy: 0.9294\n",
      "Epoch 501/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2084 - accuracy: 0.9263\n",
      "Epoch 502/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2064 - accuracy: 0.9268\n",
      "Epoch 503/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1870 - accuracy: 0.9347\n",
      "Epoch 504/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1947 - accuracy: 0.9308\n",
      "Epoch 505/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1923 - accuracy: 0.9343\n",
      "Epoch 506/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1853 - accuracy: 0.9371\n",
      "Epoch 507/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1907 - accuracy: 0.9337\n",
      "Epoch 508/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2157 - accuracy: 0.9257\n",
      "Epoch 509/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1974 - accuracy: 0.9317\n",
      "Epoch 510/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1934 - accuracy: 0.9337\n",
      "Epoch 511/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1909 - accuracy: 0.9344\n",
      "Epoch 512/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2045 - accuracy: 0.9297\n",
      "Epoch 513/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2012 - accuracy: 0.9318\n",
      "Epoch 514/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2011 - accuracy: 0.9309\n",
      "Epoch 515/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1860 - accuracy: 0.9338\n",
      "Epoch 516/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1862 - accuracy: 0.9339\n",
      "Epoch 517/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1804 - accuracy: 0.9368\n",
      "Epoch 518/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1987 - accuracy: 0.9324\n",
      "Epoch 519/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2018 - accuracy: 0.9296\n",
      "Epoch 520/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2082 - accuracy: 0.9274\n",
      "Epoch 521/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2021 - accuracy: 0.9331\n",
      "Epoch 522/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2014 - accuracy: 0.9328\n",
      "Epoch 523/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2011 - accuracy: 0.9318\n",
      "Epoch 524/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1908 - accuracy: 0.9337\n",
      "Epoch 525/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1878 - accuracy: 0.9344\n",
      "Epoch 526/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2023 - accuracy: 0.9308\n",
      "Epoch 527/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2005 - accuracy: 0.9294\n",
      "Epoch 528/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1878 - accuracy: 0.9344\n",
      "Epoch 529/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1912 - accuracy: 0.9336\n",
      "Epoch 530/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1885 - accuracy: 0.9364\n",
      "Epoch 531/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2024 - accuracy: 0.9311\n",
      "Epoch 532/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1942 - accuracy: 0.9292\n",
      "Epoch 533/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1986 - accuracy: 0.9317\n",
      "Epoch 534/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1945 - accuracy: 0.9334\n",
      "Epoch 535/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1955 - accuracy: 0.9333\n",
      "Epoch 536/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2054 - accuracy: 0.9292\n",
      "Epoch 537/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1902 - accuracy: 0.9338\n",
      "Epoch 538/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1980 - accuracy: 0.9324\n",
      "Epoch 539/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2066 - accuracy: 0.9273\n",
      "Epoch 540/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2107 - accuracy: 0.9273\n",
      "Epoch 541/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1833 - accuracy: 0.9364\n",
      "Epoch 542/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2066 - accuracy: 0.9272\n",
      "Epoch 543/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1944 - accuracy: 0.9367\n",
      "Epoch 544/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1875 - accuracy: 0.9369\n",
      "Epoch 545/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1918 - accuracy: 0.9352\n",
      "Epoch 546/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1909 - accuracy: 0.9342\n",
      "Epoch 547/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1877 - accuracy: 0.9347\n",
      "Epoch 548/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1842 - accuracy: 0.9358\n",
      "Epoch 549/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1813 - accuracy: 0.9361\n",
      "Epoch 550/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2035 - accuracy: 0.9276\n",
      "Epoch 551/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2149 - accuracy: 0.9268\n",
      "Epoch 552/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1973 - accuracy: 0.9306\n",
      "Epoch 553/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1801 - accuracy: 0.9347\n",
      "Epoch 554/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1821 - accuracy: 0.9344\n",
      "Epoch 555/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2026 - accuracy: 0.9291\n",
      "Epoch 556/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1998 - accuracy: 0.9276\n",
      "Epoch 557/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1820 - accuracy: 0.9372\n",
      "Epoch 558/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1913 - accuracy: 0.9343\n",
      "Epoch 559/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1863 - accuracy: 0.9369\n",
      "Epoch 560/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1918 - accuracy: 0.9346\n",
      "Epoch 561/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2001 - accuracy: 0.9337\n",
      "Epoch 562/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1914 - accuracy: 0.9329\n",
      "Epoch 563/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1889 - accuracy: 0.9349\n",
      "Epoch 564/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1978 - accuracy: 0.9303\n",
      "Epoch 565/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1926 - accuracy: 0.9319\n",
      "Epoch 566/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1897 - accuracy: 0.9343\n",
      "Epoch 567/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1893 - accuracy: 0.9324\n",
      "Epoch 568/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1876 - accuracy: 0.9321\n",
      "Epoch 569/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1951 - accuracy: 0.9312\n",
      "Epoch 570/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1866 - accuracy: 0.9351\n",
      "Epoch 571/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1858 - accuracy: 0.9348\n",
      "Epoch 572/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1873 - accuracy: 0.9334\n",
      "Epoch 573/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1889 - accuracy: 0.9328\n",
      "Epoch 574/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1918 - accuracy: 0.9329\n",
      "Epoch 575/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1994 - accuracy: 0.9307\n",
      "Epoch 576/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1822 - accuracy: 0.9399\n",
      "Epoch 577/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1889 - accuracy: 0.9348\n",
      "Epoch 578/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2030 - accuracy: 0.9283\n",
      "Epoch 579/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1925 - accuracy: 0.9328\n",
      "Epoch 580/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1943 - accuracy: 0.9326\n",
      "Epoch 581/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1788 - accuracy: 0.9361\n",
      "Epoch 582/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1960 - accuracy: 0.9336\n",
      "Epoch 583/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1818 - accuracy: 0.9338\n",
      "Epoch 584/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1911 - accuracy: 0.9337\n",
      "Epoch 585/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1799 - accuracy: 0.9376\n",
      "Epoch 586/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1959 - accuracy: 0.9326\n",
      "Epoch 587/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1856 - accuracy: 0.9352\n",
      "Epoch 588/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1866 - accuracy: 0.9347\n",
      "Epoch 589/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1852 - accuracy: 0.9344\n",
      "Epoch 590/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1968 - accuracy: 0.9318\n",
      "Epoch 591/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1978 - accuracy: 0.9333\n",
      "Epoch 592/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1817 - accuracy: 0.9357\n",
      "Epoch 593/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1816 - accuracy: 0.9341\n",
      "Epoch 594/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1851 - accuracy: 0.9341\n",
      "Epoch 595/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1961 - accuracy: 0.9328\n",
      "Epoch 596/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1923 - accuracy: 0.9322\n",
      "Epoch 597/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1967 - accuracy: 0.9312\n",
      "Epoch 598/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1939 - accuracy: 0.9318\n",
      "Epoch 599/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1823 - accuracy: 0.9376\n",
      "Epoch 600/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1803 - accuracy: 0.9364\n",
      "Epoch 601/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1829 - accuracy: 0.9361\n",
      "Epoch 602/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1903 - accuracy: 0.9338\n",
      "Epoch 603/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1838 - accuracy: 0.9344\n",
      "Epoch 604/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1749 - accuracy: 0.9378\n",
      "Epoch 605/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1853 - accuracy: 0.9354\n",
      "Epoch 606/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1913 - accuracy: 0.9331\n",
      "Epoch 607/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1875 - accuracy: 0.9324\n",
      "Epoch 608/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1895 - accuracy: 0.9322\n",
      "Epoch 609/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1938 - accuracy: 0.9357\n",
      "Epoch 610/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1874 - accuracy: 0.9358\n",
      "Epoch 611/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1864 - accuracy: 0.9363\n",
      "Epoch 612/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1873 - accuracy: 0.9336\n",
      "Epoch 613/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1955 - accuracy: 0.9317\n",
      "Epoch 614/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1920 - accuracy: 0.9322\n",
      "Epoch 615/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1951 - accuracy: 0.9349\n",
      "Epoch 616/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1766 - accuracy: 0.9367\n",
      "Epoch 617/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1724 - accuracy: 0.9391\n",
      "Epoch 618/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1862 - accuracy: 0.9329\n",
      "Epoch 619/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1994 - accuracy: 0.9328\n",
      "Epoch 620/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1924 - accuracy: 0.9352\n",
      "Epoch 621/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1894 - accuracy: 0.9344\n",
      "Epoch 622/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1912 - accuracy: 0.9356\n",
      "Epoch 623/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1766 - accuracy: 0.9403\n",
      "Epoch 624/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1887 - accuracy: 0.9312\n",
      "Epoch 625/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1899 - accuracy: 0.9327\n",
      "Epoch 626/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1805 - accuracy: 0.9376\n",
      "Epoch 627/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1844 - accuracy: 0.9349\n",
      "Epoch 628/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1814 - accuracy: 0.9371\n",
      "Epoch 629/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1783 - accuracy: 0.9398\n",
      "Epoch 630/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1933 - accuracy: 0.9323\n",
      "Epoch 631/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1883 - accuracy: 0.9358\n",
      "Epoch 632/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1890 - accuracy: 0.9319\n",
      "Epoch 633/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1824 - accuracy: 0.9374\n",
      "Epoch 634/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1856 - accuracy: 0.9381\n",
      "Epoch 635/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1798 - accuracy: 0.9357\n",
      "Epoch 636/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1830 - accuracy: 0.9368\n",
      "Epoch 637/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1809 - accuracy: 0.9393\n",
      "Epoch 638/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1708 - accuracy: 0.9402\n",
      "Epoch 639/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1774 - accuracy: 0.9378\n",
      "Epoch 640/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1770 - accuracy: 0.9409\n",
      "Epoch 641/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1975 - accuracy: 0.9317\n",
      "Epoch 642/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1999 - accuracy: 0.9273\n",
      "Epoch 643/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1932 - accuracy: 0.9334\n",
      "Epoch 644/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1911 - accuracy: 0.9342\n",
      "Epoch 645/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1845 - accuracy: 0.9321\n",
      "Epoch 646/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1713 - accuracy: 0.9401\n",
      "Epoch 647/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1878 - accuracy: 0.9352\n",
      "Epoch 648/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1816 - accuracy: 0.9347\n",
      "Epoch 649/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1858 - accuracy: 0.9349\n",
      "Epoch 650/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1778 - accuracy: 0.9389\n",
      "Epoch 651/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1841 - accuracy: 0.9371\n",
      "Epoch 652/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1929 - accuracy: 0.9356\n",
      "Epoch 653/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1879 - accuracy: 0.9343\n",
      "Epoch 654/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1758 - accuracy: 0.9382\n",
      "Epoch 655/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1731 - accuracy: 0.9411\n",
      "Epoch 656/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1798 - accuracy: 0.9377\n",
      "Epoch 657/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1765 - accuracy: 0.9379\n",
      "Epoch 658/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1897 - accuracy: 0.9322\n",
      "Epoch 659/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1823 - accuracy: 0.9372\n",
      "Epoch 660/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1817 - accuracy: 0.9379\n",
      "Epoch 661/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1920 - accuracy: 0.9341\n",
      "Epoch 662/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1828 - accuracy: 0.9352\n",
      "Epoch 663/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1799 - accuracy: 0.9374\n",
      "Epoch 664/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1821 - accuracy: 0.9396\n",
      "Epoch 665/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1806 - accuracy: 0.9367\n",
      "Epoch 666/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1794 - accuracy: 0.9372\n",
      "Epoch 667/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1961 - accuracy: 0.9294\n",
      "Epoch 668/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1694 - accuracy: 0.9396\n",
      "Epoch 669/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1760 - accuracy: 0.9386\n",
      "Epoch 670/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1880 - accuracy: 0.9359\n",
      "Epoch 671/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1908 - accuracy: 0.9338\n",
      "Epoch 672/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1758 - accuracy: 0.9416\n",
      "Epoch 673/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1713 - accuracy: 0.9414\n",
      "Epoch 674/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1821 - accuracy: 0.9369\n",
      "Epoch 675/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1784 - accuracy: 0.9382\n",
      "Epoch 676/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1974 - accuracy: 0.9311\n",
      "Epoch 677/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1787 - accuracy: 0.9391\n",
      "Epoch 678/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1937 - accuracy: 0.9327\n",
      "Epoch 679/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1786 - accuracy: 0.9401\n",
      "Epoch 680/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1842 - accuracy: 0.9356\n",
      "Epoch 681/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1833 - accuracy: 0.9372\n",
      "Epoch 682/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1808 - accuracy: 0.9378\n",
      "Epoch 683/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1887 - accuracy: 0.9359\n",
      "Epoch 684/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1778 - accuracy: 0.9396\n",
      "Epoch 685/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1801 - accuracy: 0.9344\n",
      "Epoch 686/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1777 - accuracy: 0.9394\n",
      "Epoch 687/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1830 - accuracy: 0.9376\n",
      "Epoch 688/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1828 - accuracy: 0.9339\n",
      "Epoch 689/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1970 - accuracy: 0.9332\n",
      "Epoch 690/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1741 - accuracy: 0.9394\n",
      "Epoch 691/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1841 - accuracy: 0.9332\n",
      "Epoch 692/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1875 - accuracy: 0.9389\n",
      "Epoch 693/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1899 - accuracy: 0.9333\n",
      "Epoch 694/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1899 - accuracy: 0.9347\n",
      "Epoch 695/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1922 - accuracy: 0.9349\n",
      "Epoch 696/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1859 - accuracy: 0.9374\n",
      "Epoch 697/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1742 - accuracy: 0.9402\n",
      "Epoch 698/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1867 - accuracy: 0.9329\n",
      "Epoch 699/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1947 - accuracy: 0.9296\n",
      "Epoch 700/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1875 - accuracy: 0.9361\n",
      "Epoch 701/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1803 - accuracy: 0.9382\n",
      "Epoch 702/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1737 - accuracy: 0.9412\n",
      "Epoch 703/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1810 - accuracy: 0.9369\n",
      "Epoch 704/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1854 - accuracy: 0.9361\n",
      "Epoch 705/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1693 - accuracy: 0.9446\n",
      "Epoch 706/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1736 - accuracy: 0.9418\n",
      "Epoch 707/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1697 - accuracy: 0.9377\n",
      "Epoch 708/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1883 - accuracy: 0.9346\n",
      "Epoch 709/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1796 - accuracy: 0.9363\n",
      "Epoch 710/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1773 - accuracy: 0.9388\n",
      "Epoch 711/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1855 - accuracy: 0.9357\n",
      "Epoch 712/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1749 - accuracy: 0.9403\n",
      "Epoch 713/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1859 - accuracy: 0.9344\n",
      "Epoch 714/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1833 - accuracy: 0.9384\n",
      "Epoch 715/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1918 - accuracy: 0.9328\n",
      "Epoch 716/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1632 - accuracy: 0.9432\n",
      "Epoch 717/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1780 - accuracy: 0.9378\n",
      "Epoch 718/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1942 - accuracy: 0.9347\n",
      "Epoch 719/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1813 - accuracy: 0.9353\n",
      "Epoch 720/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1773 - accuracy: 0.9397\n",
      "Epoch 721/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1759 - accuracy: 0.9387\n",
      "Epoch 722/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1774 - accuracy: 0.9406\n",
      "Epoch 723/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1701 - accuracy: 0.9401\n",
      "Epoch 724/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1895 - accuracy: 0.9333\n",
      "Epoch 725/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1928 - accuracy: 0.9336\n",
      "Epoch 726/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1857 - accuracy: 0.9366\n",
      "Epoch 727/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1742 - accuracy: 0.9404\n",
      "Epoch 728/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1699 - accuracy: 0.9403\n",
      "Epoch 729/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1725 - accuracy: 0.9384\n",
      "Epoch 730/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1733 - accuracy: 0.9408\n",
      "Epoch 731/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1684 - accuracy: 0.9413\n",
      "Epoch 732/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1858 - accuracy: 0.9362\n",
      "Epoch 733/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1865 - accuracy: 0.9352\n",
      "Epoch 734/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1831 - accuracy: 0.9367\n",
      "Epoch 735/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1767 - accuracy: 0.9389\n",
      "Epoch 736/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1741 - accuracy: 0.9392\n",
      "Epoch 737/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1699 - accuracy: 0.9397\n",
      "Epoch 738/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1829 - accuracy: 0.9411\n",
      "Epoch 739/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1738 - accuracy: 0.9404\n",
      "Epoch 740/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1817 - accuracy: 0.9382\n",
      "Epoch 741/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1882 - accuracy: 0.9352\n",
      "Epoch 742/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1788 - accuracy: 0.9367\n",
      "Epoch 743/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1776 - accuracy: 0.9401\n",
      "Epoch 744/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1782 - accuracy: 0.9349\n",
      "Epoch 745/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1871 - accuracy: 0.9372\n",
      "Epoch 746/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1743 - accuracy: 0.9381\n",
      "Epoch 747/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1728 - accuracy: 0.9392\n",
      "Epoch 748/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1774 - accuracy: 0.9398\n",
      "Epoch 749/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1806 - accuracy: 0.9377\n",
      "Epoch 750/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1804 - accuracy: 0.9384\n",
      "Epoch 751/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1775 - accuracy: 0.9371\n",
      "Epoch 752/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1925 - accuracy: 0.9324\n",
      "Epoch 753/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1827 - accuracy: 0.9343\n",
      "Epoch 754/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1717 - accuracy: 0.9377\n",
      "Epoch 755/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1794 - accuracy: 0.9357\n",
      "Epoch 756/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1779 - accuracy: 0.9362\n",
      "Epoch 757/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1763 - accuracy: 0.9382\n",
      "Epoch 758/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1799 - accuracy: 0.9393\n",
      "Epoch 759/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1702 - accuracy: 0.9404\n",
      "Epoch 760/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1779 - accuracy: 0.9371\n",
      "Epoch 761/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1732 - accuracy: 0.9388\n",
      "Epoch 762/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1741 - accuracy: 0.9392\n",
      "Epoch 763/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1762 - accuracy: 0.9388\n",
      "Epoch 764/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1757 - accuracy: 0.9366\n",
      "Epoch 765/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1841 - accuracy: 0.9369\n",
      "Epoch 766/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1780 - accuracy: 0.9389\n",
      "Epoch 767/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1769 - accuracy: 0.9394\n",
      "Epoch 768/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1645 - accuracy: 0.9459\n",
      "Epoch 769/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1805 - accuracy: 0.9393\n",
      "Epoch 770/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1677 - accuracy: 0.9421\n",
      "Epoch 771/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1801 - accuracy: 0.9352\n",
      "Epoch 772/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1709 - accuracy: 0.9421\n",
      "Epoch 773/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1690 - accuracy: 0.9404\n",
      "Epoch 774/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1711 - accuracy: 0.9397\n",
      "Epoch 775/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1833 - accuracy: 0.9353\n",
      "Epoch 776/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1736 - accuracy: 0.9399\n",
      "Epoch 777/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1696 - accuracy: 0.9406\n",
      "Epoch 778/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1695 - accuracy: 0.9409\n",
      "Epoch 779/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1796 - accuracy: 0.9394\n",
      "Epoch 780/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1700 - accuracy: 0.9371\n",
      "Epoch 781/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1671 - accuracy: 0.9427\n",
      "Epoch 782/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1732 - accuracy: 0.9392\n",
      "Epoch 783/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1667 - accuracy: 0.9422\n",
      "Epoch 784/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1707 - accuracy: 0.9438\n",
      "Epoch 785/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1690 - accuracy: 0.9409\n",
      "Epoch 786/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1754 - accuracy: 0.9399\n",
      "Epoch 787/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1815 - accuracy: 0.9336\n",
      "Epoch 788/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1814 - accuracy: 0.9348\n",
      "Epoch 789/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1838 - accuracy: 0.9379\n",
      "Epoch 790/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1750 - accuracy: 0.9387\n",
      "Epoch 791/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1780 - accuracy: 0.9388\n",
      "Epoch 792/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1758 - accuracy: 0.9399\n",
      "Epoch 793/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1750 - accuracy: 0.9401\n",
      "Epoch 794/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1745 - accuracy: 0.9361\n",
      "Epoch 795/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1711 - accuracy: 0.9406\n",
      "Epoch 796/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1618 - accuracy: 0.9452\n",
      "Epoch 797/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1765 - accuracy: 0.9384\n",
      "Epoch 798/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1747 - accuracy: 0.9369\n",
      "Epoch 799/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1587 - accuracy: 0.9456\n",
      "Epoch 800/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1768 - accuracy: 0.9384\n",
      "Epoch 801/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1647 - accuracy: 0.9424\n",
      "Epoch 802/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1785 - accuracy: 0.9367\n",
      "Epoch 803/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1714 - accuracy: 0.9382\n",
      "Epoch 804/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1735 - accuracy: 0.9421\n",
      "Epoch 805/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1777 - accuracy: 0.9393\n",
      "Epoch 806/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1773 - accuracy: 0.9407\n",
      "Epoch 807/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1807 - accuracy: 0.9374\n",
      "Epoch 808/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1760 - accuracy: 0.9376\n",
      "Epoch 809/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1692 - accuracy: 0.9408\n",
      "Epoch 810/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1696 - accuracy: 0.9423\n",
      "Epoch 811/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1755 - accuracy: 0.9407\n",
      "Epoch 812/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1801 - accuracy: 0.9383\n",
      "Epoch 813/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1643 - accuracy: 0.9429\n",
      "Epoch 814/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1645 - accuracy: 0.9433\n",
      "Epoch 815/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1776 - accuracy: 0.9379\n",
      "Epoch 816/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1841 - accuracy: 0.9352\n",
      "Epoch 817/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1833 - accuracy: 0.9371\n",
      "Epoch 818/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1793 - accuracy: 0.9356\n",
      "Epoch 819/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1775 - accuracy: 0.9386\n",
      "Epoch 820/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1882 - accuracy: 0.9364\n",
      "Epoch 821/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1637 - accuracy: 0.9414\n",
      "Epoch 822/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1720 - accuracy: 0.9409\n",
      "Epoch 823/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1732 - accuracy: 0.9398\n",
      "Epoch 824/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1650 - accuracy: 0.9434\n",
      "Epoch 825/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1608 - accuracy: 0.9442\n",
      "Epoch 826/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1726 - accuracy: 0.9361\n",
      "Epoch 827/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1763 - accuracy: 0.9369\n",
      "Epoch 828/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1668 - accuracy: 0.9413\n",
      "Epoch 829/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1740 - accuracy: 0.9387\n",
      "Epoch 830/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1795 - accuracy: 0.9333\n",
      "Epoch 831/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1761 - accuracy: 0.9394\n",
      "Epoch 832/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1792 - accuracy: 0.9364\n",
      "Epoch 833/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1687 - accuracy: 0.9416\n",
      "Epoch 834/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1848 - accuracy: 0.9383\n",
      "Epoch 835/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1668 - accuracy: 0.9419\n",
      "Epoch 836/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1685 - accuracy: 0.9411\n",
      "Epoch 837/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1736 - accuracy: 0.9402\n",
      "Epoch 838/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1773 - accuracy: 0.9384\n",
      "Epoch 839/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1744 - accuracy: 0.9369\n",
      "Epoch 840/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1686 - accuracy: 0.9402\n",
      "Epoch 841/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1676 - accuracy: 0.9421\n",
      "Epoch 842/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1625 - accuracy: 0.9416\n",
      "Epoch 843/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1743 - accuracy: 0.9406\n",
      "Epoch 844/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1731 - accuracy: 0.9421\n",
      "Epoch 845/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1706 - accuracy: 0.9418\n",
      "Epoch 846/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1815 - accuracy: 0.9404\n",
      "Epoch 847/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1700 - accuracy: 0.9408\n",
      "Epoch 848/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1688 - accuracy: 0.9417\n",
      "Epoch 849/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1687 - accuracy: 0.9401\n",
      "Epoch 850/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1893 - accuracy: 0.9362\n",
      "Epoch 851/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1742 - accuracy: 0.9399\n",
      "Epoch 852/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1687 - accuracy: 0.9417\n",
      "Epoch 853/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1662 - accuracy: 0.9426\n",
      "Epoch 854/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1689 - accuracy: 0.9409\n",
      "Epoch 855/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1736 - accuracy: 0.9397\n",
      "Epoch 856/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1760 - accuracy: 0.9409\n",
      "Epoch 857/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1667 - accuracy: 0.9422\n",
      "Epoch 858/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1476 - accuracy: 0.9486\n",
      "Epoch 859/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1729 - accuracy: 0.9396\n",
      "Epoch 860/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1689 - accuracy: 0.9421\n",
      "Epoch 861/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1717 - accuracy: 0.9384\n",
      "Epoch 862/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1782 - accuracy: 0.9386\n",
      "Epoch 863/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1825 - accuracy: 0.9361\n",
      "Epoch 864/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1759 - accuracy: 0.9402\n",
      "Epoch 865/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1671 - accuracy: 0.9439\n",
      "Epoch 866/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1609 - accuracy: 0.9422\n",
      "Epoch 867/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1732 - accuracy: 0.9407\n",
      "Epoch 868/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1806 - accuracy: 0.9341\n",
      "Epoch 869/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1637 - accuracy: 0.9433\n",
      "Epoch 870/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1859 - accuracy: 0.9349\n",
      "Epoch 871/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1632 - accuracy: 0.9432\n",
      "Epoch 872/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1590 - accuracy: 0.9432\n",
      "Epoch 873/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1774 - accuracy: 0.9381\n",
      "Epoch 874/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1640 - accuracy: 0.9426\n",
      "Epoch 875/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1734 - accuracy: 0.9396\n",
      "Epoch 876/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1657 - accuracy: 0.9418\n",
      "Epoch 877/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1695 - accuracy: 0.9418\n",
      "Epoch 878/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1686 - accuracy: 0.9421\n",
      "Epoch 879/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1731 - accuracy: 0.9433\n",
      "Epoch 880/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1762 - accuracy: 0.9394\n",
      "Epoch 881/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1681 - accuracy: 0.9411\n",
      "Epoch 882/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1734 - accuracy: 0.9373\n",
      "Epoch 883/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1772 - accuracy: 0.9394\n",
      "Epoch 884/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1720 - accuracy: 0.9402\n",
      "Epoch 885/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1715 - accuracy: 0.9402\n",
      "Epoch 886/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1685 - accuracy: 0.9427\n",
      "Epoch 887/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1628 - accuracy: 0.9403\n",
      "Epoch 888/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1734 - accuracy: 0.9411\n",
      "Epoch 889/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1746 - accuracy: 0.9393\n",
      "Epoch 890/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1628 - accuracy: 0.9424\n",
      "Epoch 891/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1636 - accuracy: 0.9418\n",
      "Epoch 892/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1693 - accuracy: 0.9421\n",
      "Epoch 893/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1620 - accuracy: 0.9437\n",
      "Epoch 894/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1634 - accuracy: 0.9444\n",
      "Epoch 895/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1615 - accuracy: 0.9458\n",
      "Epoch 896/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1616 - accuracy: 0.9436\n",
      "Epoch 897/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1672 - accuracy: 0.9383\n",
      "Epoch 898/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1679 - accuracy: 0.9421\n",
      "Epoch 899/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1605 - accuracy: 0.9434\n",
      "Epoch 900/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1597 - accuracy: 0.9439\n",
      "Epoch 901/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1579 - accuracy: 0.9459\n",
      "Epoch 902/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1742 - accuracy: 0.9416\n",
      "Epoch 903/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1722 - accuracy: 0.9393\n",
      "Epoch 904/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1716 - accuracy: 0.9402\n",
      "Epoch 905/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1703 - accuracy: 0.9413\n",
      "Epoch 906/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1788 - accuracy: 0.9372\n",
      "Epoch 907/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1585 - accuracy: 0.9438\n",
      "Epoch 908/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1733 - accuracy: 0.9387\n",
      "Epoch 909/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1700 - accuracy: 0.9441\n",
      "Epoch 910/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1679 - accuracy: 0.9436\n",
      "Epoch 911/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1702 - accuracy: 0.9412\n",
      "Epoch 912/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1688 - accuracy: 0.9391\n",
      "Epoch 913/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1640 - accuracy: 0.9421\n",
      "Epoch 914/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1795 - accuracy: 0.9406\n",
      "Epoch 915/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1710 - accuracy: 0.9428\n",
      "Epoch 916/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1670 - accuracy: 0.9411\n",
      "Epoch 917/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1677 - accuracy: 0.9411\n",
      "Epoch 918/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1698 - accuracy: 0.9397\n",
      "Epoch 919/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1770 - accuracy: 0.9408\n",
      "Epoch 920/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1531 - accuracy: 0.9462\n",
      "Epoch 921/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1668 - accuracy: 0.9401\n",
      "Epoch 922/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1579 - accuracy: 0.9448\n",
      "Epoch 923/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1653 - accuracy: 0.9456\n",
      "Epoch 924/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1703 - accuracy: 0.9391\n",
      "Epoch 925/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1732 - accuracy: 0.9389\n",
      "Epoch 926/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1566 - accuracy: 0.9443\n",
      "Epoch 927/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1617 - accuracy: 0.9427\n",
      "Epoch 928/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1762 - accuracy: 0.9362\n",
      "Epoch 929/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1646 - accuracy: 0.9433\n",
      "Epoch 930/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1643 - accuracy: 0.9408\n",
      "Epoch 931/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1638 - accuracy: 0.9446\n",
      "Epoch 932/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1632 - accuracy: 0.9446\n",
      "Epoch 933/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1632 - accuracy: 0.9426\n",
      "Epoch 934/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1610 - accuracy: 0.9459\n",
      "Epoch 935/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1778 - accuracy: 0.9374\n",
      "Epoch 936/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1649 - accuracy: 0.9423\n",
      "Epoch 937/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1778 - accuracy: 0.9411\n",
      "Epoch 938/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1631 - accuracy: 0.9447\n",
      "Epoch 939/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1599 - accuracy: 0.9436\n",
      "Epoch 940/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1670 - accuracy: 0.9411\n",
      "Epoch 941/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1654 - accuracy: 0.9397\n",
      "Epoch 942/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1639 - accuracy: 0.9423\n",
      "Epoch 943/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1648 - accuracy: 0.9431\n",
      "Epoch 944/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1753 - accuracy: 0.9402\n",
      "Epoch 945/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1712 - accuracy: 0.9399\n",
      "Epoch 946/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1713 - accuracy: 0.9426\n",
      "Epoch 947/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1698 - accuracy: 0.9437\n",
      "Epoch 948/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1609 - accuracy: 0.9409\n",
      "Epoch 949/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1680 - accuracy: 0.9414\n",
      "Epoch 950/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1657 - accuracy: 0.9414\n",
      "Epoch 951/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1718 - accuracy: 0.9398\n",
      "Epoch 952/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1665 - accuracy: 0.9429\n",
      "Epoch 953/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1801 - accuracy: 0.9416\n",
      "Epoch 954/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1617 - accuracy: 0.9426\n",
      "Epoch 955/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1729 - accuracy: 0.9399\n",
      "Epoch 956/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1779 - accuracy: 0.9398\n",
      "Epoch 957/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1628 - accuracy: 0.9442\n",
      "Epoch 958/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1700 - accuracy: 0.9396\n",
      "Epoch 959/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1615 - accuracy: 0.9429\n",
      "Epoch 960/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1692 - accuracy: 0.9417\n",
      "Epoch 961/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1649 - accuracy: 0.9431\n",
      "Epoch 962/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1694 - accuracy: 0.9381\n",
      "Epoch 963/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1515 - accuracy: 0.9449\n",
      "Epoch 964/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1661 - accuracy: 0.9438\n",
      "Epoch 965/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1839 - accuracy: 0.9367\n",
      "Epoch 966/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1646 - accuracy: 0.9427\n",
      "Epoch 967/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1504 - accuracy: 0.9510\n",
      "Epoch 968/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1607 - accuracy: 0.9447\n",
      "Epoch 969/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1582 - accuracy: 0.9448\n",
      "Epoch 970/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1602 - accuracy: 0.9453\n",
      "Epoch 971/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1706 - accuracy: 0.9411\n",
      "Epoch 972/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1758 - accuracy: 0.9369\n",
      "Epoch 973/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1643 - accuracy: 0.9429\n",
      "Epoch 974/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1557 - accuracy: 0.9474\n",
      "Epoch 975/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1707 - accuracy: 0.9407\n",
      "Epoch 976/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1701 - accuracy: 0.9422\n",
      "Epoch 977/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1735 - accuracy: 0.9374\n",
      "Epoch 978/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1652 - accuracy: 0.9416\n",
      "Epoch 979/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1724 - accuracy: 0.9388\n",
      "Epoch 980/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1663 - accuracy: 0.9424\n",
      "Epoch 981/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1597 - accuracy: 0.9468\n",
      "Epoch 982/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1756 - accuracy: 0.9397\n",
      "Epoch 983/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1615 - accuracy: 0.9456\n",
      "Epoch 984/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1674 - accuracy: 0.9433\n",
      "Epoch 985/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1649 - accuracy: 0.9426\n",
      "Epoch 986/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1721 - accuracy: 0.9402\n",
      "Epoch 987/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1600 - accuracy: 0.9448\n",
      "Epoch 988/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1759 - accuracy: 0.9402\n",
      "Epoch 989/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1651 - accuracy: 0.9444\n",
      "Epoch 990/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1778 - accuracy: 0.9398\n",
      "Epoch 991/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1592 - accuracy: 0.9457\n",
      "Epoch 992/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1678 - accuracy: 0.9423\n",
      "Epoch 993/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1720 - accuracy: 0.9404\n",
      "Epoch 994/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1683 - accuracy: 0.9423\n",
      "Epoch 995/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1750 - accuracy: 0.9422\n",
      "Epoch 996/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1583 - accuracy: 0.9472\n",
      "Epoch 997/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1649 - accuracy: 0.9454\n",
      "Epoch 998/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1626 - accuracy: 0.9439\n",
      "Epoch 999/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1624 - accuracy: 0.9447\n",
      "Epoch 1000/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1737 - accuracy: 0.9412\n",
      "Epoch 1001/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1664 - accuracy: 0.9439\n",
      "Epoch 1002/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1645 - accuracy: 0.9437\n",
      "Epoch 1003/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1586 - accuracy: 0.9433\n",
      "Epoch 1004/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1578 - accuracy: 0.9454\n",
      "Epoch 1005/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1766 - accuracy: 0.9358\n",
      "Epoch 1006/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1462 - accuracy: 0.9489\n",
      "Epoch 1007/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1701 - accuracy: 0.9413\n",
      "Epoch 1008/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1600 - accuracy: 0.9464\n",
      "Epoch 1009/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1678 - accuracy: 0.9409\n",
      "Epoch 1010/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1641 - accuracy: 0.9456\n",
      "Epoch 1011/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1764 - accuracy: 0.9399\n",
      "Epoch 1012/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1694 - accuracy: 0.9418\n",
      "Epoch 1013/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1481 - accuracy: 0.9491\n",
      "Epoch 1014/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1715 - accuracy: 0.9388\n",
      "Epoch 1015/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1666 - accuracy: 0.9442\n",
      "Epoch 1016/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1680 - accuracy: 0.9403\n",
      "Epoch 1017/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1633 - accuracy: 0.9442\n",
      "Epoch 1018/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1697 - accuracy: 0.9418\n",
      "Epoch 1019/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1608 - accuracy: 0.9447\n",
      "Epoch 1020/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1601 - accuracy: 0.9453\n",
      "Epoch 1021/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1557 - accuracy: 0.9457\n",
      "Epoch 1022/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1697 - accuracy: 0.9417\n",
      "Epoch 1023/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1732 - accuracy: 0.9412\n",
      "Epoch 1024/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1659 - accuracy: 0.9418\n",
      "Epoch 1025/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1614 - accuracy: 0.9444\n",
      "Epoch 1026/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1593 - accuracy: 0.9432\n",
      "Epoch 1027/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1702 - accuracy: 0.9411\n",
      "Epoch 1028/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1638 - accuracy: 0.9406\n",
      "Epoch 1029/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1602 - accuracy: 0.9452\n",
      "Epoch 1030/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1607 - accuracy: 0.9446\n",
      "Epoch 1031/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1562 - accuracy: 0.9487\n",
      "Epoch 1032/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1634 - accuracy: 0.9436\n",
      "Epoch 1033/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1550 - accuracy: 0.9459\n",
      "Epoch 1034/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1766 - accuracy: 0.9411\n",
      "Epoch 1035/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1629 - accuracy: 0.9439\n",
      "Epoch 1036/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1574 - accuracy: 0.9443\n",
      "Epoch 1037/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1536 - accuracy: 0.9471\n",
      "Epoch 1038/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1712 - accuracy: 0.9429\n",
      "Epoch 1039/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1577 - accuracy: 0.9482\n",
      "Epoch 1040/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1547 - accuracy: 0.9454\n",
      "Epoch 1041/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1644 - accuracy: 0.9444\n",
      "Epoch 1042/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1536 - accuracy: 0.9467\n",
      "Epoch 1043/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1636 - accuracy: 0.9432\n",
      "Epoch 1044/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1684 - accuracy: 0.9423\n",
      "Epoch 1045/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1624 - accuracy: 0.9444\n",
      "Epoch 1046/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1645 - accuracy: 0.9413\n",
      "Epoch 1047/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1656 - accuracy: 0.9427\n",
      "Epoch 1048/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1640 - accuracy: 0.9403\n",
      "Epoch 1049/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1760 - accuracy: 0.9373\n",
      "Epoch 1050/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1594 - accuracy: 0.9456\n",
      "Epoch 1051/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1486 - accuracy: 0.9508\n",
      "Epoch 1052/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1651 - accuracy: 0.9432\n",
      "Epoch 1053/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1680 - accuracy: 0.9421\n",
      "Epoch 1054/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1626 - accuracy: 0.9451\n",
      "Epoch 1055/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1654 - accuracy: 0.9417\n",
      "Epoch 1056/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1616 - accuracy: 0.9448\n",
      "Epoch 1057/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1620 - accuracy: 0.9422\n",
      "Epoch 1058/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1525 - accuracy: 0.9484\n",
      "Epoch 1059/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1595 - accuracy: 0.9477\n",
      "Epoch 1060/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1715 - accuracy: 0.9403\n",
      "Epoch 1061/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1611 - accuracy: 0.9441\n",
      "Epoch 1062/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1600 - accuracy: 0.9432\n",
      "Epoch 1063/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1563 - accuracy: 0.9463\n",
      "Epoch 1064/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1635 - accuracy: 0.9436\n",
      "Epoch 1065/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1582 - accuracy: 0.9434\n",
      "Epoch 1066/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1495 - accuracy: 0.9459\n",
      "Epoch 1067/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1539 - accuracy: 0.9444\n",
      "Epoch 1068/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1670 - accuracy: 0.9439\n",
      "Epoch 1069/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1586 - accuracy: 0.9421\n",
      "Epoch 1070/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1550 - accuracy: 0.9469\n",
      "Epoch 1071/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1482 - accuracy: 0.9498\n",
      "Epoch 1072/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1649 - accuracy: 0.9414\n",
      "Epoch 1073/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1749 - accuracy: 0.9401\n",
      "Epoch 1074/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1629 - accuracy: 0.9441\n",
      "Epoch 1075/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1690 - accuracy: 0.9412\n",
      "Epoch 1076/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1671 - accuracy: 0.9384\n",
      "Epoch 1077/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1686 - accuracy: 0.9399\n",
      "Epoch 1078/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1644 - accuracy: 0.9431\n",
      "Epoch 1079/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1491 - accuracy: 0.9488\n",
      "Epoch 1080/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1564 - accuracy: 0.9457\n",
      "Epoch 1081/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1633 - accuracy: 0.9406\n",
      "Epoch 1082/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1544 - accuracy: 0.9459\n",
      "Epoch 1083/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1492 - accuracy: 0.9483\n",
      "Epoch 1084/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1509 - accuracy: 0.9448\n",
      "Epoch 1085/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1621 - accuracy: 0.9427\n",
      "Epoch 1086/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1657 - accuracy: 0.9437\n",
      "Epoch 1087/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1621 - accuracy: 0.9441\n",
      "Epoch 1088/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1683 - accuracy: 0.9388\n",
      "Epoch 1089/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1559 - accuracy: 0.9454\n",
      "Epoch 1090/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1492 - accuracy: 0.9510\n",
      "Epoch 1091/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1572 - accuracy: 0.9452\n",
      "Epoch 1092/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1563 - accuracy: 0.9457\n",
      "Epoch 1093/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1675 - accuracy: 0.9426\n",
      "Epoch 1094/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1519 - accuracy: 0.9508\n",
      "Epoch 1095/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1639 - accuracy: 0.9434\n",
      "Epoch 1096/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1749 - accuracy: 0.9402\n",
      "Epoch 1097/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1637 - accuracy: 0.9416\n",
      "Epoch 1098/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1690 - accuracy: 0.9429\n",
      "Epoch 1099/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1611 - accuracy: 0.9464\n",
      "Epoch 1100/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1724 - accuracy: 0.9452\n",
      "Epoch 1101/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1648 - accuracy: 0.9446\n",
      "Epoch 1102/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1628 - accuracy: 0.9434\n",
      "Epoch 1103/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1553 - accuracy: 0.9463\n",
      "Epoch 1104/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1750 - accuracy: 0.9406\n",
      "Epoch 1105/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1665 - accuracy: 0.9431\n",
      "Epoch 1106/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1583 - accuracy: 0.9446\n",
      "Epoch 1107/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1500 - accuracy: 0.9484\n",
      "Epoch 1108/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1553 - accuracy: 0.9466\n",
      "Epoch 1109/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1513 - accuracy: 0.9471\n",
      "Epoch 1110/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1624 - accuracy: 0.9427\n",
      "Epoch 1111/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1620 - accuracy: 0.9423\n",
      "Epoch 1112/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1664 - accuracy: 0.9436\n",
      "Epoch 1113/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1745 - accuracy: 0.9397\n",
      "Epoch 1114/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1614 - accuracy: 0.9454\n",
      "Epoch 1115/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1587 - accuracy: 0.9443\n",
      "Epoch 1116/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1563 - accuracy: 0.9468\n",
      "Epoch 1117/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1551 - accuracy: 0.9457\n",
      "Epoch 1118/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1477 - accuracy: 0.9497\n",
      "Epoch 1119/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1654 - accuracy: 0.9439\n",
      "Epoch 1120/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1592 - accuracy: 0.9439\n",
      "Epoch 1121/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1513 - accuracy: 0.9479\n",
      "Epoch 1122/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1552 - accuracy: 0.9453\n",
      "Epoch 1123/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1705 - accuracy: 0.9419\n",
      "Epoch 1124/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1486 - accuracy: 0.9477\n",
      "Epoch 1125/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1589 - accuracy: 0.9466\n",
      "Epoch 1126/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1533 - accuracy: 0.9487\n",
      "Epoch 1127/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1606 - accuracy: 0.9442\n",
      "Epoch 1128/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1597 - accuracy: 0.9432\n",
      "Epoch 1129/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1634 - accuracy: 0.9412\n",
      "Epoch 1130/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1595 - accuracy: 0.9426\n",
      "Epoch 1131/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1639 - accuracy: 0.9411\n",
      "Epoch 1132/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1508 - accuracy: 0.9484\n",
      "Epoch 1133/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1662 - accuracy: 0.9431\n",
      "Epoch 1134/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1727 - accuracy: 0.9414\n",
      "Epoch 1135/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1508 - accuracy: 0.9477\n",
      "Epoch 1136/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1565 - accuracy: 0.9454\n",
      "Epoch 1137/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1548 - accuracy: 0.9479\n",
      "Epoch 1138/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1729 - accuracy: 0.9423\n",
      "Epoch 1139/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1555 - accuracy: 0.9419\n",
      "Epoch 1140/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1485 - accuracy: 0.9473\n",
      "Epoch 1141/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1539 - accuracy: 0.9478\n",
      "Epoch 1142/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1700 - accuracy: 0.9417\n",
      "Epoch 1143/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1601 - accuracy: 0.9436\n",
      "Epoch 1144/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1570 - accuracy: 0.9463\n",
      "Epoch 1145/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1577 - accuracy: 0.9427\n",
      "Epoch 1146/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1597 - accuracy: 0.9481\n",
      "Epoch 1147/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1661 - accuracy: 0.9401\n",
      "Epoch 1148/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1512 - accuracy: 0.9464\n",
      "Epoch 1149/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1597 - accuracy: 0.9454\n",
      "Epoch 1150/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1633 - accuracy: 0.9446\n",
      "Epoch 1151/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1703 - accuracy: 0.9411\n",
      "Epoch 1152/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1595 - accuracy: 0.9433\n",
      "Epoch 1153/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1749 - accuracy: 0.9399\n",
      "Epoch 1154/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1551 - accuracy: 0.9481\n",
      "Epoch 1155/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1664 - accuracy: 0.9419\n",
      "Epoch 1156/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1590 - accuracy: 0.9459\n",
      "Epoch 1157/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1684 - accuracy: 0.9446\n",
      "Epoch 1158/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1617 - accuracy: 0.9428\n",
      "Epoch 1159/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1693 - accuracy: 0.9398\n",
      "Epoch 1160/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1485 - accuracy: 0.9453\n",
      "Epoch 1161/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1532 - accuracy: 0.9468\n",
      "Epoch 1162/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1584 - accuracy: 0.9433\n",
      "Epoch 1163/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1560 - accuracy: 0.9461\n",
      "Epoch 1164/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1351 - accuracy: 0.9523\n",
      "Epoch 1165/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1616 - accuracy: 0.9433\n",
      "Epoch 1166/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1659 - accuracy: 0.9388\n",
      "Epoch 1167/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1503 - accuracy: 0.9466\n",
      "Epoch 1168/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1538 - accuracy: 0.9459\n",
      "Epoch 1169/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1508 - accuracy: 0.9471\n",
      "Epoch 1170/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1556 - accuracy: 0.9466\n",
      "Epoch 1171/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1620 - accuracy: 0.9446\n",
      "Epoch 1172/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1637 - accuracy: 0.9444\n",
      "Epoch 1173/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1634 - accuracy: 0.9457\n",
      "Epoch 1174/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1544 - accuracy: 0.9463\n",
      "Epoch 1175/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1569 - accuracy: 0.9477\n",
      "Epoch 1176/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1615 - accuracy: 0.9408\n",
      "Epoch 1177/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1641 - accuracy: 0.9431\n",
      "Epoch 1178/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1591 - accuracy: 0.9429\n",
      "Epoch 1179/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1653 - accuracy: 0.9406\n",
      "Epoch 1180/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1536 - accuracy: 0.9474\n",
      "Epoch 1181/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1555 - accuracy: 0.9444\n",
      "Epoch 1182/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1553 - accuracy: 0.9459\n",
      "Epoch 1183/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1632 - accuracy: 0.9438\n",
      "Epoch 1184/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1625 - accuracy: 0.9426\n",
      "Epoch 1185/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1519 - accuracy: 0.9452\n",
      "Epoch 1186/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1546 - accuracy: 0.9461\n",
      "Epoch 1187/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1609 - accuracy: 0.9427\n",
      "Epoch 1188/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1684 - accuracy: 0.9406\n",
      "Epoch 1189/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1595 - accuracy: 0.9454\n",
      "Epoch 1190/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1650 - accuracy: 0.9408\n",
      "Epoch 1191/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1654 - accuracy: 0.9423\n",
      "Epoch 1192/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1589 - accuracy: 0.9451\n",
      "Epoch 1193/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1565 - accuracy: 0.9458\n",
      "Epoch 1194/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1591 - accuracy: 0.9428\n",
      "Epoch 1195/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1702 - accuracy: 0.9442\n",
      "Epoch 1196/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1541 - accuracy: 0.9456\n",
      "Epoch 1197/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1531 - accuracy: 0.9488\n",
      "Epoch 1198/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1670 - accuracy: 0.9431\n",
      "Epoch 1199/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1523 - accuracy: 0.9477\n",
      "Epoch 1200/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1523 - accuracy: 0.9481\n",
      "Epoch 1201/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1672 - accuracy: 0.9426\n",
      "Epoch 1202/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1588 - accuracy: 0.9436\n",
      "Epoch 1203/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1582 - accuracy: 0.9441\n",
      "Epoch 1204/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1485 - accuracy: 0.9463\n",
      "Epoch 1205/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1547 - accuracy: 0.9476\n",
      "Epoch 1206/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1664 - accuracy: 0.9467\n",
      "Epoch 1207/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1540 - accuracy: 0.9453\n",
      "Epoch 1208/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1503 - accuracy: 0.9469\n",
      "Epoch 1209/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1465 - accuracy: 0.9483\n",
      "Epoch 1210/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1596 - accuracy: 0.9424\n",
      "Epoch 1211/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1564 - accuracy: 0.9484\n",
      "Epoch 1212/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1566 - accuracy: 0.9473\n",
      "Epoch 1213/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1561 - accuracy: 0.9477\n",
      "Epoch 1214/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1568 - accuracy: 0.9459\n",
      "Epoch 1215/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1523 - accuracy: 0.9491\n",
      "Epoch 1216/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1569 - accuracy: 0.9462\n",
      "Epoch 1217/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1601 - accuracy: 0.9451\n",
      "Epoch 1218/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1670 - accuracy: 0.9413\n",
      "Epoch 1219/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1575 - accuracy: 0.9452\n",
      "Epoch 1220/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1622 - accuracy: 0.9434\n",
      "Epoch 1221/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1526 - accuracy: 0.9467\n",
      "Epoch 1222/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1643 - accuracy: 0.9416\n",
      "Epoch 1223/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1539 - accuracy: 0.9458\n",
      "Epoch 1224/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1583 - accuracy: 0.9463\n",
      "Epoch 1225/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1607 - accuracy: 0.9431\n",
      "Epoch 1226/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1568 - accuracy: 0.9481\n",
      "Epoch 1227/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1479 - accuracy: 0.9467\n",
      "Epoch 1228/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1558 - accuracy: 0.9441\n",
      "Epoch 1229/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1675 - accuracy: 0.9427\n",
      "Epoch 1230/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1612 - accuracy: 0.9454\n",
      "Epoch 1231/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1557 - accuracy: 0.9471\n",
      "Epoch 1232/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1522 - accuracy: 0.9468\n",
      "Epoch 1233/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1562 - accuracy: 0.9477\n",
      "Epoch 1234/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1554 - accuracy: 0.9481\n",
      "Epoch 1235/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1602 - accuracy: 0.9444\n",
      "Epoch 1236/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1532 - accuracy: 0.9469\n",
      "Epoch 1237/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1520 - accuracy: 0.9449\n",
      "Epoch 1238/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1561 - accuracy: 0.9447\n",
      "Epoch 1239/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1498 - accuracy: 0.9477\n",
      "Epoch 1240/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1500 - accuracy: 0.9511\n",
      "Epoch 1241/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1490 - accuracy: 0.9489\n",
      "Epoch 1242/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1631 - accuracy: 0.9443\n",
      "Epoch 1243/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1571 - accuracy: 0.9463\n",
      "Epoch 1244/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1583 - accuracy: 0.9454\n",
      "Epoch 1245/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1427 - accuracy: 0.9532\n",
      "Epoch 1246/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1503 - accuracy: 0.9482\n",
      "Epoch 1247/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1543 - accuracy: 0.9442\n",
      "Epoch 1248/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1596 - accuracy: 0.9439\n",
      "Epoch 1249/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1637 - accuracy: 0.9422\n",
      "Epoch 1250/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1575 - accuracy: 0.9479\n",
      "Epoch 1251/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1544 - accuracy: 0.9448\n",
      "Epoch 1252/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1515 - accuracy: 0.9496\n",
      "Epoch 1253/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1543 - accuracy: 0.9486\n",
      "Epoch 1254/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1632 - accuracy: 0.9431\n",
      "Epoch 1255/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1498 - accuracy: 0.9459\n",
      "Epoch 1256/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1494 - accuracy: 0.9502\n",
      "Epoch 1257/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1556 - accuracy: 0.9468\n",
      "Epoch 1258/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1612 - accuracy: 0.9429\n",
      "Epoch 1259/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1572 - accuracy: 0.9459\n",
      "Epoch 1260/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1596 - accuracy: 0.9453\n",
      "Epoch 1261/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1513 - accuracy: 0.9461\n",
      "Epoch 1262/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1493 - accuracy: 0.9482\n",
      "Epoch 1263/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1460 - accuracy: 0.9501\n",
      "Epoch 1264/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1538 - accuracy: 0.9492\n",
      "Epoch 1265/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1528 - accuracy: 0.9458\n",
      "Epoch 1266/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1494 - accuracy: 0.9496\n",
      "Epoch 1267/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1584 - accuracy: 0.9439\n",
      "Epoch 1268/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1627 - accuracy: 0.9424\n",
      "Epoch 1269/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1492 - accuracy: 0.9502\n",
      "Epoch 1270/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1583 - accuracy: 0.9474\n",
      "Epoch 1271/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1449 - accuracy: 0.9497\n",
      "Epoch 1272/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1498 - accuracy: 0.9506\n",
      "Epoch 1273/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1570 - accuracy: 0.9463\n",
      "Epoch 1274/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1580 - accuracy: 0.9462\n",
      "Epoch 1275/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1567 - accuracy: 0.9461\n",
      "Epoch 1276/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1574 - accuracy: 0.9454\n",
      "Epoch 1277/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1517 - accuracy: 0.9472\n",
      "Epoch 1278/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1655 - accuracy: 0.9452\n",
      "Epoch 1279/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1611 - accuracy: 0.9447\n",
      "Epoch 1280/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1516 - accuracy: 0.9505\n",
      "Epoch 1281/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1410 - accuracy: 0.9498\n",
      "Epoch 1282/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1571 - accuracy: 0.9459\n",
      "Epoch 1283/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1478 - accuracy: 0.9512\n",
      "Epoch 1284/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1503 - accuracy: 0.9503\n",
      "Epoch 1285/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1636 - accuracy: 0.9464\n",
      "Epoch 1286/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1608 - accuracy: 0.9449\n",
      "Epoch 1287/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1651 - accuracy: 0.9418\n",
      "Epoch 1288/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1613 - accuracy: 0.9411\n",
      "Epoch 1289/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1576 - accuracy: 0.9434\n",
      "Epoch 1290/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1507 - accuracy: 0.9482\n",
      "Epoch 1291/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1463 - accuracy: 0.9481\n",
      "Epoch 1292/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1658 - accuracy: 0.9412\n",
      "Epoch 1293/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1591 - accuracy: 0.9477\n",
      "Epoch 1294/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1520 - accuracy: 0.9489\n",
      "Epoch 1295/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1548 - accuracy: 0.9447\n",
      "Epoch 1296/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1588 - accuracy: 0.9434\n",
      "Epoch 1297/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1495 - accuracy: 0.9483\n",
      "Epoch 1298/1300\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.1594 - accuracy: 0.9432\n",
      "Epoch 1299/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1583 - accuracy: 0.9443\n",
      "Epoch 1300/1300\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1528 - accuracy: 0.9476\n"
     ]
    }
   ],
   "source": [
    "model = build_mlnn(\n",
    "    hidden_dim=best[\"hidden_dim\"],\n",
    "    dropout_rate=best[\"dropout_rate\"]\n",
    ")\n",
    "\n",
    "model = mlnn_compile(\n",
    "    model,\n",
    "    learning_rate=best[\"learning_rate\"]\n",
    ")\n",
    "\n",
    "model = mlnn_fit(\n",
    "    model,\n",
    "    x_train_scaled,\n",
    "    y_train,\n",
    "    val_xy=None,\n",
    "    epochs=1300,\n",
    "    batch_size=best[\"batch_size\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/djq98242/repos/genre_classification/models/best_mlnn_all_training/best_mlnn_all_training/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\n",
    "    get_project_root() / \"models/best_mlnn_all_training/best_mlnn_all_training\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## performance on test set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_results = pd.read_pickle(\n",
    "    get_project_root() / \"output/gs_results_20221217.pkl\"\n",
    ")\n",
    "\n",
    "best = gs_results[gs_results[\"accuracy\"] == gs_results[\"accuracy\"].max()]\n",
    "best = best[\"params\"].reset_index(drop=True)[0]\n",
    "best = ast.literal_eval(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_dim': 128,\n",
       " 'dropout_rate': 0.1,\n",
       " 'learning_rate': 0.001,\n",
       " 'batch_size': 32}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-12 14:33:30.584728: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\n",
    "    get_project_root() / \"models/best_mlnn_all_training/best_mlnn_all_training\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mlnn_genre_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (Dense)               (None, 32)                1856      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 32)               128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " hidden1 (Dense)             (None, 128)               4224      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " hidden2 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " hidden3 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42,058\n",
      "Trainable params: 41,226\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the x_test with the same scaler as used on the training set:\n",
    "scale_cols = x_test.columns\n",
    "x_test_scaled = scaler.transform(x_test[scale_cols])\n",
    "# retrieve column names for scaled df:\n",
    "x_test_scaled = pd.DataFrame(\n",
    "    x_test_scaled,\n",
    "    columns=scale_cols\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* would be good practice to code this into a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_class = np.argmax(pred, axis=1)\n",
    "pred_class = pd.Series(pred_class)\n",
    "# pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_class = pred_class.map(catno_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[197,   0,   4,   1,   0,   3,   0,   0,   1,   2],\n",
       "       [  0, 199,   0,   0,   0,   4,   0,   0,   0,   0],\n",
       "       [  0,   2, 178,   0,   0,   3,   0,   2,   1,   0],\n",
       "       [  1,   4,   1, 191,   1,   0,   0,   0,   0,   1],\n",
       "       [  1,   0,   1,   2, 209,   0,   0,   2,   2,   1],\n",
       "       [  1,   3,   0,   0,   0, 188,   0,   0,   0,   0],\n",
       "       [  2,   0,   2,   0,   0,   0, 197,   0,   0,   3],\n",
       "       [  0,   1,   0,   1,   1,   2,   0, 171,   3,   1],\n",
       "       [  0,   1,   2,   2,   4,   0,   0,   0, 201,   1],\n",
       "       [  2,   1,   1,   1,   1,   0,   3,   1,   1, 186]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(\n",
    "    y_test,\n",
    "    pred_class\n",
    ")\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x15b1b4220>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConfusionMatrixDisplay(\n",
    "    cm,\n",
    "    display_labels=label_to_catno.keys()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(7,7))\n",
    "ax = fig.add_subplot(111)\n",
    "ConfusionMatrixDisplay(\n",
    "    cm,\n",
    "    display_labels=label_to_catno.keys()\n",
    ").plot(\n",
    "    xticks_rotation=90,\n",
    "    cmap=\"Blues\",\n",
    "    colorbar=False,\n",
    "    ax=ax\n",
    ")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\n",
    "    get_project_root() / \"output/mlnn/best_mlnn_all_training_cm.svg\"\n",
    ")\n",
    "fig.savefig(\n",
    "    get_project_root() / \"output/mlnn/best_mlnn_all_training_cm.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_metrics(y_test, y_pred):\n",
    "    scorers = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"f1_micro\": f1_score(y_test, y_pred, average=\"micro\"),\n",
    "        \"f1_macro\": f1_score(y_test, y_pred, average=\"macro\"),\n",
    "        \"recall_micro\": recall_score(y_test, y_pred, average=\"micro\"),\n",
    "        \"recall_macro\": recall_score(y_test, y_pred, average=\"macro\"),\n",
    "        \"precision_micro\": precision_score(y_test, y_pred, average=\"micro\"),\n",
    "        \"precision_macro\": precision_score(y_test, y_pred, average=\"macro\")\n",
    "    }\n",
    "    df = pd.DataFrame(index=scorers.keys(), columns=[\"metric score\"])\n",
    "    for scorer in scorers.keys():\n",
    "        df.at[scorer, \"metric score\"] = scorers[scorer]\n",
    "    df=df.transpose()\n",
    "    return df\n",
    "\n",
    "def class_metrics(y_test, y_pred, class_labels):\n",
    "    df = pd.DataFrame(index=list(class_labels))\n",
    "\n",
    "    for scorer in [\n",
    "        (\"recall\", recall_score),\n",
    "        (\"precision\", precision_score),\n",
    "        (\"f1\", f1_score)\n",
    "    ]:\n",
    "        metric_score = scorer[1](\n",
    "            y_test,\n",
    "            y_pred,\n",
    "            average=None,\n",
    "            labels=list(class_labels)\n",
    "        )\n",
    "        df[scorer[0]] = metric_score\n",
    "\n",
    "    df = df.transpose()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>recall_micro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>precision_micro</th>\n",
       "      <th>precision_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>metric score</th>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.959357</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.959454</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.959501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              accuracy  f1_micro  f1_macro recall_micro recall_macro  \\\n",
       "metric score  0.959459  0.959459  0.959357     0.959459     0.959454   \n",
       "\n",
       "             precision_micro precision_macro  \n",
       "metric score        0.959459        0.959501  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "av_metrics = average_metrics(y_test, pred_class)\n",
    "av_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0112/143402.479401:INFO:headless_shell.cc(223)] 19822 bytes written to file /var/folders/ng/4x8hyf_s39b2rqzs184hl_lr0000gp/T/tmplw0od0ga/temp.png\n"
     ]
    }
   ],
   "source": [
    "dfi.export(\n",
    "    av_metrics,\n",
    "    \"../output/mlnn/best_mlnn_all_training_average_metrics.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0112/143405.414898:INFO:headless_shell.cc(223)] 31214 bytes written to file /var/folders/ng/4x8hyf_s39b2rqzs184hl_lr0000gp/T/tmp50jebumw/temp.png\n"
     ]
    }
   ],
   "source": [
    "av_metrics_t = av_metrics.transpose()\n",
    "dfi.export(\n",
    "    av_metrics_t,\n",
    "    \"../output/mlnn/best_mlnn_all_training_average_metrics_t.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blues</th>\n",
       "      <th>classical</th>\n",
       "      <th>country</th>\n",
       "      <th>disco</th>\n",
       "      <th>hiphop</th>\n",
       "      <th>jazz</th>\n",
       "      <th>metal</th>\n",
       "      <th>pop</th>\n",
       "      <th>reggae</th>\n",
       "      <th>rock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.947115</td>\n",
       "      <td>0.980296</td>\n",
       "      <td>0.956989</td>\n",
       "      <td>0.959799</td>\n",
       "      <td>0.958716</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.965686</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.952607</td>\n",
       "      <td>0.944162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.965686</td>\n",
       "      <td>0.943128</td>\n",
       "      <td>0.941799</td>\n",
       "      <td>0.964646</td>\n",
       "      <td>0.967593</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.971591</td>\n",
       "      <td>0.961722</td>\n",
       "      <td>0.953846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.956311</td>\n",
       "      <td>0.961353</td>\n",
       "      <td>0.949333</td>\n",
       "      <td>0.962217</td>\n",
       "      <td>0.963134</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.975248</td>\n",
       "      <td>0.960674</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.948980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              blues  classical   country     disco    hiphop      jazz  \\\n",
       "recall     0.947115   0.980296  0.956989  0.959799  0.958716  0.979167   \n",
       "precision  0.965686   0.943128  0.941799  0.964646  0.967593  0.940000   \n",
       "f1         0.956311   0.961353  0.949333  0.962217  0.963134  0.959184   \n",
       "\n",
       "              metal       pop    reggae      rock  \n",
       "recall     0.965686  0.950000  0.952607  0.944162  \n",
       "precision  0.985000  0.971591  0.961722  0.953846  \n",
       "f1         0.975248  0.960674  0.957143  0.948980  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_metrics = class_metrics(y_test, pred_class, label_to_catno.keys())\n",
    "c_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0112/143419.427153:INFO:headless_shell.cc(223)] 39766 bytes written to file /var/folders/ng/4x8hyf_s39b2rqzs184hl_lr0000gp/T/tmpxf78a8rw/temp.png\n"
     ]
    }
   ],
   "source": [
    "dfi.export(\n",
    "    c_metrics,\n",
    "    \"../output/mlnn/best_mlnn_all_training_class_metrics.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0112/143422.626571:INFO:headless_shell.cc(223)] 57549 bytes written to file /var/folders/ng/4x8hyf_s39b2rqzs184hl_lr0000gp/T/tmp6uzxsk7d/temp.png\n"
     ]
    }
   ],
   "source": [
    "c_metrics_t = c_metrics.transpose()\n",
    "dfi.export(\n",
    "    c_metrics_t,\n",
    "    \"../output/mlnn/best_mlnn_all_training_class_metrics_t.png\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'mlnn_genre_classification',\n",
       " 'layers': [{'class_name': 'InputLayer',\n",
       "   'config': {'batch_input_shape': (None, 57),\n",
       "    'dtype': 'float32',\n",
       "    'sparse': False,\n",
       "    'ragged': False,\n",
       "    'name': 'input_input'}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'input',\n",
       "    'trainable': True,\n",
       "    'batch_input_shape': (None, 57),\n",
       "    'dtype': 'float32',\n",
       "    'units': 32,\n",
       "    'activation': 'selu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
       "     'config': {'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': ListWrapper([1]),\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None}},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_4',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.1,\n",
       "    'noise_shape': None,\n",
       "    'seed': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'hidden1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 128,\n",
       "    'activation': 'selu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
       "     'config': {'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': ListWrapper([1]),\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None}},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.1,\n",
       "    'noise_shape': None,\n",
       "    'seed': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'hidden2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 128,\n",
       "    'activation': 'selu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
       "     'config': {'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': ListWrapper([1]),\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None}},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.1,\n",
       "    'noise_shape': None,\n",
       "    'seed': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'hidden3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 128,\n",
       "    'activation': 'selu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
       "     'config': {'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'BatchNormalization',\n",
       "   'config': {'name': 'batch_normalization_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'axis': ListWrapper([1]),\n",
       "    'momentum': 0.99,\n",
       "    'epsilon': 0.001,\n",
       "    'center': True,\n",
       "    'scale': True,\n",
       "    'beta_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'gamma_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'moving_variance_initializer': {'class_name': 'Ones', 'config': {}},\n",
       "    'beta_regularizer': None,\n",
       "    'gamma_regularizer': None,\n",
       "    'beta_constraint': None,\n",
       "    'gamma_constraint': None}},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.1,\n",
       "    'noise_shape': None,\n",
       "    'seed': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'output',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 10,\n",
       "    'activation': 'softmax',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
       "     'config': {'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}}]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'optimizer': <keras.optimizers.optimizer_v2.adam.Adam at 0x15af86e80>,\n",
       " 'loss': <function keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1)>,\n",
       " 'metrics': [[<keras.metrics.base_metric.MeanMetricWrapper at 0x15afa2b50>]],\n",
       " 'weighted_metrics': None,\n",
       " 'loss_weights': None}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._get_compile_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genreclassification-qGJVucn0-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "071c4954f4b056e381337a2b30bebca67942706fcde46b495279624c5d47332a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
